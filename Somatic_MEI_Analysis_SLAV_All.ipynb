{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import glob, os, gc\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from __future__ import division\n",
    "from time import time\n",
    "from subprocess import (call, Popen, PIPE)\n",
    "from itertools import product\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "from sklearn.decomposition import (PCA, RandomizedPCA)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import shutil\n",
    "import pybedtools\n",
    "import pysam\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "##Path to Data\n",
    "basepath = \"/home/ubuntu/efs/Common_Experiment/final\" \n",
    "narrowpeak = \"-ready_peaks.narrowPeak\"\n",
    "peaks_merged = \"_peaksMerged.txt\"\n",
    "peaks_merged_bed = \"_peaksMerged.bed\"\n",
    "peaks_correct_bed = \"_peaksCorrect.bed\"\n",
    "peakregions_sml = \".peakregions_sml\"\n",
    "peakregions_lrg = \".peakregions_lrg\"\n",
    "peaks_correct_data = \"_peaksCorrect.data\"\n",
    "peaks_L1HS_bedgraph = \"_peaks_L1HS_mapped.bedgraph\"\n",
    "loci_sml = \".loci_sml\"\n",
    "loci_lrg = \".loci_lrg\"\n",
    "overlap = \"_overlap_\"\n",
    "overlap_sml = \"_overlap_sml_\"\n",
    "overlap_lrg = \"_overlap_lrg_\"\n",
    "L1HS_bam = \"-L1HS_mapped.bam\"\n",
    "L1HS_bam_bai = \"-L1HS_mapped.bam.bai\"\n",
    "bam = \"-ready.bam\"\n",
    "igv = \"-igv.xml\"\n",
    "bed = \".bed\"\n",
    "## rmask Paths \n",
    "L1HS = \"/home/ubuntu/efs/SLAV_Data/rmask_L1HS_Final.bed\"\n",
    "L1PA2345 = \"/home/ubuntu/efs/SLAV_Data/rmask_L1PA2345_Final.bed\"\n",
    "L1_Other = \"/home/ubuntu/efs/SLAV_Data/rmask_L1_Other_Final.bed\"\n",
    "##IGV Template\n",
    "IGV = \"/home/ubuntu/efs/SLAV_Data/igv-template4.xml\"\n",
    "\n",
    "##SLAV Data\n",
    "Bulk_1571_Cerebellum = \"1571_cereb_BT_40_L3\"\n",
    "Bulk_1571_Hippocampus = \"1571_hippo_BT_41_L3\"\n",
    "Bulk_1846_Cerebellum = \"1846_cereb_BT_13_L3\"\n",
    "Bulk_1846_Cortex = \"1846_cortex_BT_71_L3\"\n",
    "Bulk_1846_Hippocampus = \"1846_hippo_BT_19_L3\"\n",
    "Bulk_1846_Liver = \"1846_liver_BT_22_L3\"\n",
    "Bulk_5125_Cortex = \"5125_cortex_BT_122_L3\"\n",
    "Bulk_5125_Hippocampus = \"5125_hippo_BT_139_L3\"\n",
    "Bulk_5125_Liver = \"5125_liver_BT_164_L3\"\n",
    "#\n",
    "SC_1571_Hippo = [\"1571_hippo_SC_43_L3\",\"1571_hippo_SC_45_L3\",\"1571_hippo_SC_46_L3\",\"1571_hippo_SC_47_L3\",\"1571_hippo_SC_48_L3\",\"1571_hippo_SC_50_L3\",\"1571_hippo_SC_51_L3\",\"1571_hippo_SC_52_L3\",\"1571_hippo_SC_53_L3\",\"1571_hippo_SC_55_L3\",\"1571_hippo_SC_56_L3\",\"1571_hippo_SC_57_L3\",\"1571_hippo_SC_58_L3\",\"1571_hippo_SC_59_L3\",\"1571_hippo_SC_61_L3\",\"1571_hippo_SC_62_L3\",\"1571_hippo_SC_63_L3\",\"1571_hippo_SC_64_L3\"]\n",
    "SC_1846_Cortex = [\"1846_cortex_SC_72_L3\",\"1846_cortex_SC_73_L3\",\"1846_cortex_SC_74_L3\",\"1846_cortex_SC_75_L3\",\"1846_cortex_SC_78_L3\",\"1846_cortex_SC_79_L3\",\"1846_cortex_SC_80_L3\",\"1846_cortex_SC_81_L3\",\"1846_cortex_SC_82_L3\",\"1846_cortex_SC_83_L3\",\"1846_cortex_SC_84_L3\",\"1846_cortex_SC_85_L3\",\"1846_cortex_SC_86_L3\"]\n",
    "SC_1846_Hippo = [\"1846_hippo_SC_100_L3\",\"1846_hippo_SC_101_L3\",\"1846_hippo_SC_102_L3\",\"1846_hippo_SC_103_L3\",\"1846_hippo_SC_104_L3\",\"1846_hippo_SC_105_L3\",\"1846_hippo_SC_106_L3\",\"1846_hippo_SC_107_L3\",\"1846_hippo_SC_108_L3\",\"1846_hippo_SC_109_L3\",\"1846_hippo_SC_110_L3\",\"1846_hippo_SC_111_L3\",\"1846_hippo_SC_112_L3\",\"1846_hippo_SC_113_L3\",\"1846_hippo_SC_88_L3\",\"1846_hippo_SC_89_L3\",\"1846_hippo_SC_90_L3\",\"1846_hippo_SC_91_L3\",\"1846_hippo_SC_92_L3\",\"1846_hippo_SC_93_L3\",\"1846_hippo_SC_94_L3\",\"1846_hippo_SC_95_L3\",\"1846_hippo_SC_99_L3\"]\n",
    "SC_5125_Cortex = [\"5125_cortex_SC_125_L3\",\"5125_cortex_SC_126_L3\",\"5125_cortex_SC_127_L3\",\"5125_cortex_SC_128_L3\",\"5125_cortex_SC_129_L3\",\"5125_cortex_SC_130_L3\",\"5125_cortex_SC_131_L3\",\"5125_cortex_SC_132_L3\",\"5125_cortex_SC_133_L3\",\"5125_cortex_SC_134_L3\",\"5125_cortex_SC_135_L3\",\"5125_cortex_SC_136_L3\",\"5125_cortex_SC_138_L3\"]\n",
    "SC_5125_Hippo = [\"5125_hippo_SC_140_L3\",\"5125_hippo_SC_141_L3\",\"5125_hippo_SC_142_L3\",\"5125_hippo_SC_143_L3\",\"5125_hippo_SC_144_L3\",\"5125_hippo_SC_145_L3\",\"5125_hippo_SC_147_L3\",\"5125_hippo_SC_149_L3\",\"5125_hippo_SC_150_L3\",\"5125_hippo_SC_151_L3\",\"5125_hippo_SC_152_L3\",\"5125_hippo_SC_153_L3\",\"5125_hippo_SC_154_L3\",\"5125_hippo_SC_155_L3\",\"5125_hippo_SC_156_L3\",\"5125_hippo_SC_157_L3\",\"5125_hippo_SC_158_L3\",\"5125_hippo_SC_159_L3\",\"5125_hippo_SC_160_L3\",\"5125_hippo_SC_161_L3\",\"5125_hippo_SC_162_L3\",\"5125_hippo_SC_163_L3\"]\n",
    "\n",
    "##Common Data\n",
    "Bulk_Brain_Common = \"04132016_mw_Bulk_cor\"\n",
    "Bulk_Fibro_Common = \"05252016_mw_Bulk_fib\"\n",
    "SC_MDA_Common = [\"04132016_mw_1571_SC_B4_S48\",\"04132016_mw_1571_SC_D4_S49\",\"04132016_mw_L1B1_SC_A2_S43\",\"04132016_mw_L1B1_SC_C1_S45\",\"04132016_mw_L1B1_SC_C2_S46\",\"04132016_mw_L1B1_SC_D2_S50\",\"04132016_mw_L1B1_SC_E2_S51\",\"04132016_mw_L1B1_SC_E3_S52\",\"04132016_mw_L1B1_SC_E3_S52\",\"04132016_mw_L1B1_SC_F2_S53\",\"04132016_mw_L1B1_SC_G1_S54\",\"04132016_mw_L1B1_SC_H1_S55\",\"05252016_mw_L1B1_SC_B4_S47\"] \n",
    "SC_MALBAC_Common = [\"2122_S1\",\"2178_S2\",\"2179_S3\",\"2180_S4\",\"2184_S5\",\"2186_S6\",\"2187_S7\",\"2188_S8\",\"2193_S9\",\"2196_S10\",\"2197_S11\",\"2198_S12\",\"2261_S13\",\"2263_S14\",\"2264_S15\",\"2265_S16\"] \n",
    "\n",
    "\n",
    "\n",
    "Data_Sets = []\n",
    "#Data_Sets.append([SC_1571_Hippo,Bulk_1571_Hippocampus,Bulk_1571_Cerebellum])\n",
    "#Data_Sets.append([SC_1846_Cortex,Bulk_1846_Cortex,Bulk_1846_Liver])\n",
    "#Data_Sets.append([SC_1846_Hippo,Bulk_1846_Hippocampus,Bulk_1846_Liver])\n",
    "#Data_Sets.append([SC_5125_Cortex,Bulk_5125_Cortex,Bulk_5125_Liver])\n",
    "#Data_Sets.append([SC_5125_Hippo,Bulk_5125_Hippocampus,Bulk_5125_Liver])\n",
    "\n",
    "Data_Sets = []\n",
    "Data_Sets.append([SC_MDA_Common,Bulk_Brain_Common,Bulk_Fibro_Common])\n",
    "Data_Sets.append([SC_MALBAC_Common,Bulk_Brain_Common,Bulk_Fibro_Common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05252016_mw_Bulk_fib\n"
     ]
    }
   ],
   "source": [
    "## soft call peaks in bulk tissues\n",
    "for dset in Data_Sets:\n",
    "    tissue = dset[1]\n",
    "    os.chdir(os.path.join(basepath, tissue))\n",
    "    if not (glob.glob(os.path.join(basepath, tissue, \"peaks.txt\"))):\n",
    "        print tissue\n",
    "        name = os.path.join(basepath, tissue, tissue)\n",
    "        bamfile = os.path.join(basepath, tissue, tissue+bam)\n",
    "        #p1 = Popen(['makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['pos2bed.pl', '-bed', os.path.join(basepath, tissue, \"peaks.txt\")])\n",
    "        p4.wait()\n",
    "    tissue = dset[2]\n",
    "    os.chdir(os.path.join(basepath, tissue))\n",
    "    if not (glob.glob(os.path.join(basepath, tissue, \"peaks.txt\"))):\n",
    "        print tissue\n",
    "        name = os.path.join(basepath, tissue, tissue)\n",
    "        bamfile = os.path.join(basepath, tissue, tissue+bam)\n",
    "        #p1 = Popen(['makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['pos2bed.pl', '-bed', os.path.join(basepath, tissue, \"peaks.txt\")])\n",
    "        p4.wait()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04132016_mw_1571_SC_B4_S48\n",
      "04132016_mw_1571_SC_D4_S49\n",
      "04132016_mw_L1B1_SC_A2_S43\n",
      "04132016_mw_L1B1_SC_C1_S45\n",
      "04132016_mw_L1B1_SC_C2_S46\n",
      "04132016_mw_L1B1_SC_D2_S50\n",
      "04132016_mw_L1B1_SC_E2_S51\n",
      "04132016_mw_L1B1_SC_E3_S52\n",
      "04132016_mw_L1B1_SC_E3_S52\n",
      "04132016_mw_L1B1_SC_F2_S53\n",
      "04132016_mw_L1B1_SC_G1_S54\n",
      "04132016_mw_L1B1_SC_H1_S55\n",
      "05252016_mw_L1B1_SC_B4_S47\n",
      "2122_S1\n",
      "2178_S2\n",
      "2179_S3\n",
      "2180_S4\n",
      "2184_S5\n",
      "2186_S6\n",
      "2187_S7\n",
      "2188_S8\n",
      "2193_S9\n",
      "2196_S10\n",
      "2197_S11\n",
      "2198_S12\n",
      "2261_S13\n",
      "2263_S14\n",
      "2264_S15\n",
      "2265_S16\n"
     ]
    }
   ],
   "source": [
    "## soft call peaks in single cells\n",
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, cell))\n",
    "        name = os.path.join(basepath, cell, cell)\n",
    "        bamfile = os.path.join(basepath, cell, cell+bam)\n",
    "        #p1 = Popen(['/home/preed/homer/bin/makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['pos2bed.pl', '-bed', os.path.join(basepath, cell, \"peaks.txt\")])\n",
    "        p4.wait()\n",
    "        p5 = Popen(['mergePeaks', '-code', '-prefix', cell, os.path.join(basepath, cell, \"peaks.txt\"), os.path.join(basepath, dset[1], \"peaks.txt\"), os.path.join(basepath,  dset[2], \"peaks.txt\")])\n",
    "        p5.wait()\n",
    "        #myoutput = open(os.path.join(basepath, cell, cell + peaks_merged), 'w')\n",
    "        #p5 = Popen(['/home/preed/homer/bin/mergePeaks', os.path.join(basepath, cell, \"peaks.txt\"), os.path.join(basepath, dset[1], \"peaks.txt\"), os.path.join(basepath,  dset[2], \"peaks.txt\")], stdout=myoutput)\n",
    "        #p5.wait()\n",
    "        myoutput = open(os.path.join(basepath, cell, cell + peaks_merged), 'w')\n",
    "        p6 = Popen(['mergePeaks', os.path.join(basepath,  cell, cell+\"_100\"),os.path.join(basepath,  cell, cell+\"_110\"),os.path.join(basepath,  cell, cell+\"_111\")], stdout=myoutput)\n",
    "        p6.wait()\n",
    "        p7 = Popen(['pos2bed.pl', os.path.join(basepath,  cell, cell + peaks_merged), '-o', os.path.join(basepath,  cell, cell+peaks_merged_bed)])\n",
    "        p7.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04132016_mw_1571_SC_B4_S48\n",
      "04132016_mw_1571_SC_D4_S49\n",
      "04132016_mw_L1B1_SC_A2_S43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54ba1ff9d855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mread_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"empty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                         \u001b[0mread_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.split('.')[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                         \u001b[0mread_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##make XML file for each cell to use with igv_plotter\n",
    "for dset in Data_Sets:\n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        tree = ET.parse(IGV)\n",
    "        root = tree.getroot()\n",
    "        root[0][0].set('path', os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)) #L1HS bedgraph\n",
    "        root[0][1].set('path', os.path.join(basepath,  cell, cell + bam)) #SC Path\n",
    "        root[0][3].set('path', os.path.join(basepath,  dset[1], dset[1] + bam)) #Bulk Brain Path\n",
    "        root[0][6].set('path', os.path.join(basepath,  dset[2], dset[2] + bam)) #Bulk Fib Path\n",
    "        root[1][0].set('id', os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)) #L1HS bedgraph\n",
    "        root[2][0].set('id', os.path.join(basepath,  cell, cell + bam)) #SC Path\n",
    "        root[3][0].set('id', os.path.join(basepath,  dset[1], dset[1] + bam)) #Bulk Brain path\n",
    "        root[4][0].set('id', os.path.join(basepath,  dset[2], dset[2] + bam)) #Bulk Fib Path\n",
    "        tree.write(os.path.join(basepath,  cell, cell + igv))\n",
    "\n",
    "\n",
    "\n",
    "        myinput = open(os.path.join(basepath,  cell, cell + peaks_merged_bed))\n",
    "        myoutput = open(os.path.join(basepath,  cell, cell + peaks_correct_bed), 'w')\n",
    "        proc3 = Popen(['grep', '-E', '^(1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|X|Y)'], stdin=myinput, stdout=myoutput)\n",
    "        proc3.wait()    \n",
    "\n",
    "        myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "        myoutput1 = os.path.join(basepath,  cell, cell + peakregions_sml)\n",
    "        myoutput2 = os.path.join(basepath,  cell, cell + peakregions_lrg)\n",
    "        with open(myoutput1, 'w') as outfile:\n",
    "            with open(myinput, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for string in data:\n",
    "                    line = string.split('\\t')\n",
    "                    pos1 = int(line[1])\n",
    "                    pos2 = int(line[2])                  \n",
    "                    center = int((pos1 + pos2)/2)\n",
    "                    pad = 1000\n",
    "                    start = center - pad\n",
    "                    end = center + pad\n",
    "                    row = [line[0], str(start), str(end)]\n",
    "                    outfile.write('\\t'.join(row) + '\\n')\n",
    "        outfile.close()\n",
    "        infile.close()   \n",
    "        with open(myoutput2, 'w') as outfile:\n",
    "            with open(myinput, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for string in data:\n",
    "                    line = string.split('\\t')\n",
    "                    pos1 = int(line[1])\n",
    "                    pos2 = int(line[2])                  \n",
    "                    center = int((pos1 + pos2)/2)\n",
    "                    pad = 10000\n",
    "                    newstart = center - pad\n",
    "                    newend = center + pad\n",
    "                    row = [line[0], str(newstart), str(newend)]\n",
    "                    outfile.write('\\t'.join(row) + '\\n')\n",
    "        outfile.close()\n",
    "        infile.close()\n",
    " \n",
    "        #make and define L1HS sub sam file here and L1HS read names list\n",
    "        \n",
    "               \n",
    "        sc_file = pysam.AlignmentFile(os.path.join(basepath,  cell, cell + bam), \"rb\")\n",
    "        bb_file = pysam.AlignmentFile(os.path.join(basepath,  dset[1], dset[1] + bam), \"rb\")\n",
    "        bf_file = pysam.AlignmentFile(os.path.join(basepath,  dset[2], dset[2] + bam), \"rb\")\n",
    "        \n",
    "\n",
    "        myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "        myoutput = os.path.join(basepath,  cell, cell + peaks_correct_data)\n",
    "        \n",
    "        with open(myoutput, 'w') as outfile:\n",
    "            with open(myinput, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for region in data:\n",
    "                    sc_iter = sc_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                    bb_iter = bb_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                    bf_iter = bf_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                    sc_i = 0\n",
    "                    bb_i = 0\n",
    "                    bf_i = 0\n",
    "                    for x in sc_iter: sc_i+=1\n",
    "                    for y in bb_iter: bb_i+=1\n",
    "                    for z in bf_iter: bf_i+=1\n",
    "                    sc_count = 1 if sc_i > 0 else 0\n",
    "                    bb_count = 1 if bb_i > 0 else 0 \n",
    "                    bf_count = 1 if bf_i > 0 else 0 \n",
    "                    row = [str(region.strip().split('\\t')[0]), str(region.strip().split('\\t')[1]), str(region.strip().split('\\t')[2])]  \n",
    "                    outfile.write('\\t'.join(row) +'\\t'+str(sc_count)+str(bb_count)+str(bf_count)+'\\n')\n",
    "        outfile.close()\n",
    "        infile.close()\n",
    "        \n",
    "        myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "        myoutput = open(os.path.join(basepath, cell, cell + L1HS_bam), 'w')\n",
    "        myoutput2 = os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)\n",
    "        p1 = Popen(['samtools', 'view', '-hb', '-@', '1', '-L', L1HS, os.path.join(basepath, cell, cell + bam)], stdout=myoutput)\n",
    "        p1.wait()\n",
    "        myoutput.close()\n",
    "        p2 = Popen(['samtools', 'index', os.path.join(basepath, cell, cell + L1HS_bam)])\n",
    "        p2.wait()\n",
    "        L1HS_file = pysam.AlignmentFile(os.path.join(basepath, cell, cell + L1HS_bam), 'rb')\n",
    "        L1HS_read_names = []\n",
    "        for read in L1HS_file.fetch(): \n",
    "            read_name = str(read).split('\\t')[0]#.split('.')[1])\n",
    "            L1HS_read_names.append(read_name)\n",
    "        with open(myoutput2, 'w') as outfile:\n",
    "            with open(myinput, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for region in data:\n",
    "                    read_names = [\"empty\"]\n",
    "                    for read in sc_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2])):\n",
    "                        read_name = str(read).split('\\t')[0]#.split('.')[1])\n",
    "                        read_names.append(read_name)\n",
    "                    a = len(read_names)\n",
    "                    L1HS_overlap = set(read_names) & set(L1HS_read_names)\n",
    "                    b = len(L1HS_overlap)\n",
    "                    percent = int((b/a)*100)\n",
    "                    row = [str(region.strip().split('\\t')[0]), str(region.strip().split('\\t')[1]), str(region.strip().split('\\t')[2]), str(percent)]  \n",
    "                    outfile.write('\\t'.join(row)+'\\n')\n",
    "        outfile.close()\n",
    "        infile.close()\n",
    "        \n",
    "        filelist =[os.path.join(basepath,  cell, \"peaks.bed\"),os.path.join(basepath,  dset[1], \"peaks.bed\"),os.path.join(basepath,  dset[2], \"peaks.bed\"),L1HS,L1PA2345,L1_Other]\n",
    "        a = pybedtools.BedTool(os.path.join(basepath, cell, cell + peaks_correct_bed)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "        count = 0\n",
    "        for fname in filelist:\n",
    "            b = pybedtools.BedTool(fname)\n",
    "            a_and_b = a.intersect(b, c=True)\n",
    "            myoutput = os.path.join(basepath,  cell, cell + overlap + str(count))\n",
    "            count +=1\n",
    "            a_and_b.saveas(myoutput)\n",
    "            myinput = open(myoutput)\n",
    "            newoutput = open(myoutput+\"_binary\", 'w')\n",
    "            #overlap_append\n",
    "            awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($7 ~ \"^[1-9]*$\") $7 = \"1\"; else $7 = $7; }; 1\"\"\"\n",
    "            proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "            proc.wait()\n",
    "            newoutput.flush()\n",
    "#         peaks = open(os.path.join(basepath, cell, cell + peaks_correct_bed))\n",
    "#         peaks_sorted = open(os.path.join(basepath, cell, cell + peaks_correct_bed_sorted), 'w')\n",
    "#         proc = Popen([\"sort\", \"-k1,1\", \"-k2,2n\"], stdin=peaks, stdout=peaks_sorted)  \n",
    "#         proc.wait()\n",
    "#         l1_input = open(os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph))\n",
    "#         l1_sorted = open(os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph_sorted), 'w')\n",
    "#         proc = Popen([\"sort\", \"-k1,1\", \"-k2,2n\"], stdin=l1_input, stdout=l1_sorted)  \n",
    "#         proc.wait()\n",
    "#         a_sorted = pybedtools.BedTool(os.path.join(basepath, cell, cell + peaks_correct_bed_sorted)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "#         l1 = pybedtools.BedTool(os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph_sorted))\n",
    "#         a_and_l1 = a_sorted.map(l1, c=4, o='sum')\n",
    "#         myoutput_l1 = os.path.join(basepath,  cell, cell + overlap + \"L1\")\n",
    "#         a_and_l1.saveas(myoutput_l1)\n",
    "        \n",
    "        a_sml = pybedtools.BedTool(os.path.join(basepath,  cell, cell + peakregions_sml)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "        count = 0\n",
    "        for fname in filelist:\n",
    "            b = pybedtools.BedTool(fname)\n",
    "            a_and_b = a_sml.intersect(b, c=True)\n",
    "            myoutput = os.path.join(basepath,  cell, cell + overlap_sml + str(count))\n",
    "            count +=1\n",
    "            a_and_b.saveas(myoutput)\n",
    "            myinput = open(myoutput)\n",
    "            newoutput = open(myoutput+\"_binary\", 'w')\n",
    "            #overlap_append\n",
    "            awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "            proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "            proc.wait()\n",
    "            newoutput.flush()\n",
    "\n",
    "        a_lrg = pybedtools.BedTool(os.path.join(basepath,  cell, cell + peakregions_lrg)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "        count = 0\n",
    "        for fname in filelist:\n",
    "            b = pybedtools.BedTool(fname)\n",
    "            a_and_b = a_lrg.intersect(b, c=True)\n",
    "            myoutput = os.path.join(basepath,  cell, cell + overlap_lrg + str(count))\n",
    "            count +=1\n",
    "            a_and_b.saveas(myoutput)\n",
    "            myinput = open(myoutput)\n",
    "            newoutput = open(myoutput+\"_binary\", 'w')\n",
    "            #overlap_append\n",
    "            awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "            proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "            proc.wait()\n",
    "            newoutput.flush()        \n",
    "\n",
    "        myinput_sml = os.path.join(basepath,  cell, cell + peakregions_sml)\n",
    "        myoutput_sml = os.path.join(basepath,  cell, cell + loci_sml)\n",
    "        with open(myoutput_sml, 'w') as outfile:\n",
    "            with open(myinput_sml, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for region in data:\n",
    "                    row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "                    outfile.write(\"\".join(row)+'\\n')\n",
    "\n",
    "        myinput_lrg = os.path.join(basepath,  cell, cell + peakregions_lrg)\n",
    "        myoutput_lrg = os.path.join(basepath,  cell, cell + loci_lrg)\n",
    "        with open(myoutput_lrg, 'w') as outfile:\n",
    "            with open(myinput_lrg, 'r') as infile:\n",
    "                data = infile.readlines()\n",
    "                for region in data:\n",
    "                    row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "                    outfile.write(\"\".join(row)+'\\n')    \n",
    "\n",
    "        Popen(['split', '-l', '100', '-d', os.path.join(basepath,  cell, cell + loci_sml), os.path.join(basepath,  cell, cell + \".split_loci_sml_\")]).wait()\n",
    "        Popen(['split', '-l', '100', '-d', os.path.join(basepath,  cell, cell + loci_lrg), os.path.join(basepath,  cell, cell + \".split_loci_lrg_\")]).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04132016_mw_1571_SC_B4_S48\n",
      "0\n",
      "04132016_mw_1571_SC_D4_S49\n",
      "0\n",
      "04132016_mw_L1B1_SC_A2_S43\n",
      "0\n",
      "04132016_mw_L1B1_SC_C1_S45\n",
      "0\n",
      "04132016_mw_L1B1_SC_C2_S46\n",
      "0\n",
      "04132016_mw_L1B1_SC_D2_S50\n",
      "0\n",
      "04132016_mw_L1B1_SC_E2_S51\n",
      "0\n",
      "04132016_mw_L1B1_SC_E3_S52\n",
      "0\n",
      "04132016_mw_L1B1_SC_E3_S52\n",
      "0\n",
      "04132016_mw_L1B1_SC_F2_S53\n",
      "0\n",
      "04132016_mw_L1B1_SC_G1_S54\n",
      "0\n",
      "04132016_mw_L1B1_SC_H1_S55\n",
      "0\n",
      "05252016_mw_L1B1_SC_B4_S47\n",
      "0\n",
      "2122_S1\n",
      "0\n",
      "2178_S2\n",
      "0\n",
      "2179_S3\n",
      "0\n",
      "2180_S4\n",
      "0\n",
      "2184_S5\n",
      "0\n",
      "2186_S6\n",
      "0\n",
      "2187_S7\n",
      "0\n",
      "2188_S8\n",
      "0\n",
      "2193_S9\n",
      "0\n",
      "2196_S10\n",
      "0\n",
      "2197_S11\n",
      "0\n",
      "2198_S12\n",
      "0\n",
      "2261_S13\n",
      "0\n",
      "2263_S14\n",
      "0\n",
      "2264_S15\n",
      "0\n",
      "2265_S16\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#generate images at each peak position small\n",
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, cell))    \n",
    "        locifile = os.path.join(basepath, cell, cell + loci_sml)\n",
    "        worklist = glob.glob(\"*.split_loci_sml_*\")\n",
    "        batchsize = 10\n",
    "        print len(worklist)\n",
    "        for i in xrange(0, len(worklist), batchsize):\n",
    "            batch = worklist[i:i+batchsize]\n",
    "            print i\n",
    "            index = 1\n",
    "            procs = []\n",
    "            for file in batch:\n",
    "                print file\n",
    "                with open(os.path.join(basepath, cell, file)) as f0:\n",
    "                    first = f0.readline()# Read the first line.\n",
    "                    for last in f0: pass\n",
    "                    firstpic = cell+\"_sml\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    lastpic = cell+\"_sml\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    if not (glob.glob(os.path.join(basepath, cell, firstpic)) or glob.glob(os.path.join(basepath, cell, lastpic))): \n",
    "                        p = Popen(['igv_plotter', '-o', cell+\"_sml_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/home/ubuntu/efs/SLAV_Data/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, cell, cell + igv)])\n",
    "                        procs.append(p)\n",
    "            for pp in procs:\n",
    "                pp.wait()\n",
    "                #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate images at each peak position lrg\n",
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, diry, cell))    \n",
    "        locifile = os.path.join(basepath, diry, cell, cell + loci_lrg)\n",
    "        worklist = glob.glob(\"*.split_loci_lrg_*\")\n",
    "        batchsize = 10\n",
    "        print len(worklist)\n",
    "        for i in xrange(0, len(worklist), batchsize):\n",
    "            batch = worklist[i:i+batchsize]\n",
    "            print i\n",
    "            index = 1\n",
    "            procs = []\n",
    "            for file in batch:\n",
    "                print file\n",
    "                with open(os.path.join(basepath, diry, cell, file)) as f0:\n",
    "                    first = f0.readline()# Read the first line.\n",
    "                    for last in f0: pass\n",
    "                    firstpic = cell+\"_lrg\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    lastpic = cell+\"_lrg\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    if not (glob.glob(os.path.join(basepath, diry, cell, firstpic)) or glob.glob(os.path.join(basepath, diry, cell, lastpic))): \n",
    "                        p = Popen(['igv_plotter', '-o', cell+\"_lrg_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/home/ubuntu/efs/SLAV_Data/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, diry, cell, cell + igv)])\n",
    "                        procs.append(p)\n",
    "            for pp in procs:\n",
    "                pp.wait()\n",
    "                #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "#         for file in glob.glob(\"*.png\"):\n",
    "#             if \"mod\" not in file:\n",
    "#                 path = os.path.splitext(file)[0]\n",
    "#                 basename = os.path.basename(path)\n",
    "#                 outfile1 = basename + \"_mod.png\"\n",
    "#                 if not os.path.isfile(os.path.join(basepath,cell,outfile1)):               \n",
    "#                     img = Image.open(file)\n",
    "#                     pixelMap = img.load()\n",
    "#                     img2 = Image.new(img.mode, img.size)\n",
    "#                     pixelsNew = img2.load()\n",
    "#                     for i in range(img2.size[0]):\n",
    "#                         for j in range(img2.size[1]):\n",
    "#                             if 250 in pixelMap[i,j]:\n",
    "#                                 pixelMap[i,j] = (0,0,0,0)\n",
    "#                             else:\n",
    "#                                 pixelsNew[i,j] = pixelMap[i,j]\n",
    "#                 img2.crop((160,130,img.size[0],img.size[1])).resize((256,256)).save(outfile1)\n",
    "\n",
    "        mergedpeak_data = os.path.join(basepath, cell, cell + peaks_correct_data)\n",
    "        regions_sml = os.path.join(basepath, cell, cell + peakregions_sml)\n",
    "        regions_lrg = os.path.join(basepath, cell, cell + peakregions_lrg)\n",
    "\n",
    "        count=1\n",
    "        with open(mergedpeak_data) as r0,open(regions_sml) as r_sml,open(regions_lrg) as r_lrg:\n",
    "            Files= {}\n",
    "            for rr0,rr_sml,rr_lrg in zip(r0,r_sml,r_lrg):\n",
    "                line = rr0.strip().split('\\t')[0]+\"\\t\"+rr0.strip().split('\\t')[1]+\"\\t\"+rr0.strip().split('\\t')[2]+\"\\t\"+cell+\"_sml-\"+rr_sml.strip().split('\\t')[0]+\"_\"+rr_sml.strip().split('\\t')[1]+\"_\"+rr_sml.strip().split('\\t')[2]+\"_blk.png\"+\"\\t\"+cell+\"_lrg-\"+rr_lrg.strip().split('\\t')[0]+\"_\"+rr_lrg.strip().split('\\t')[1]+\"_\"+rr_lrg.strip().split('\\t')[2]+\"_blk.png\"+\"\\t\"+rr0.strip().split('\\t')[3]\n",
    "                Files[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a = os.path.join(basepath, cell, cell+\"_overlap_0_binary\")\n",
    "        b = os.path.join(basepath, cell, cell+\"_overlap_1_binary\")\n",
    "        c = os.path.join(basepath, cell, cell+\"_overlap_2_binary\")\n",
    "        d = os.path.join(basepath, cell, cell+\"_overlap_3_binary\")\n",
    "        e = os.path.join(basepath, cell, cell+\"_overlap_4_binary\")\n",
    "        f = os.path.join(basepath, cell, cell+\"_overlap_5_binary\")\n",
    "        g = os.path.join(basepath, cell, cell+ peaks_L1HS_bedgraph)\n",
    "        count=1\n",
    "        with open(a) as f1,open(b) as f2,open(c) as f3,open(d) as f4,open(e) as f5,open(f) as f6,open(g) as f7:\n",
    "            Peaks = {}\n",
    "            for aa,bb,cc,dd,ee,ff,gg in zip(f1,f2,f3,f4,f5,f6,f7):\n",
    "                line = aa.strip().split('\\t')[6]+bb.strip().split('\\t')[6]+cc.strip().split('\\t')[6]+\"\\t\"+dd.strip().split('\\t')[6]+ee.strip().split('\\t')[6]+ff.strip().split('\\t')[6]+\"\\t\"+gg.strip().split('\\t')[3]\n",
    "                Peaks[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_0_binary\")\n",
    "        b_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_1_binary\")\n",
    "        c_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_2_binary\")\n",
    "        d_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_3_binary\")\n",
    "        e_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_4_binary\")\n",
    "        f_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_5_binary\")\n",
    "        count=1\n",
    "        with open(a_sml) as f1,open(b_sml) as f2,open(c_sml) as f3,open(d_sml) as f4,open(e_sml) as f5,open(f_sml) as f6:\n",
    "            Small = {}\n",
    "            for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "                line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "                Small[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_0_binary\")\n",
    "        b_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_1_binary\")\n",
    "        c_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_2_binary\")\n",
    "        d_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_3_binary\")\n",
    "        e_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_4_binary\")\n",
    "        f_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_5_binary\")\n",
    "        count=1\n",
    "        with open(a_lrg) as f1,open(b_lrg) as f2,open(c_lrg) as f3,open(d_lrg) as f4,open(e_lrg) as f5,open(f_lrg) as f6:\n",
    "            Large = {}\n",
    "            for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "                line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "                Large[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        with open(os.path.join(basepath, cell, cell+\"_Input_metadata.txt\"),\"w\") as f8:\n",
    "            for key in Files:\n",
    "                encoding = Files[key]+\":\"+Peaks[key].strip().split('\\t')[0]+\":\"+Peaks[key].strip().split('\\t')[1]+\":\"+Small[key].strip().split('\\t')[0]+\":\"+Small[key].strip().split('\\t')[1]+\":\"+Large[key].strip().split('\\t')[0]+\":\"+Large[key].strip().split('\\t')[1]+\":\"+Peaks[key].strip().split('\\t')[2]+\"\\n\"            #for (Fi, Fj),(Pi,Pj),(Si,Sj),(Li,Lj) in zip(Files.items(),Peaks.items(),Small.items(),Large.items()):\n",
    "                f8.write(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:\n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        with open(os.path.join(basepath, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "            for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "                if os.path.isfile(os.path.join(basepath, cell, line[3])):\n",
    "                    #print line[3]\n",
    "                    filename = line[3]\n",
    "                    readclass = line[5].split(\":\")[0]\n",
    "                    peakclass_peak = line[5].split(\":\")[1]\n",
    "                    L1_class_peak = line[5].split(\":\")[2]\n",
    "                    peakclass_sml = line[5].split(\":\")[3]\n",
    "                    L1_class_sml = line[5].split(\":\")[4]\n",
    "                    peakclass_lrg = line[5].split(\":\")[5]\n",
    "                    L1_class_lrg = line[5].split(\":\")[6]\n",
    "                    L1_Percent = line[5].split(\":\")[7]\n",
    "                    if 0 <= int(L1_Percent) <= 25:\n",
    "                        class_sml = [readclass,peakclass_peak,L1_class_peak,peakclass_sml,L1_class_sml,peakclass_lrg,L1_class_lrg,\"low\"]\n",
    "                    elif 25 < int(L1_Percent) < 75:\n",
    "                        class_sml = [readclass,peakclass_peak,L1_class_peak,peakclass_sml,L1_class_sml,peakclass_lrg,L1_class_lrg,\"med\"]\n",
    "                    elif 75 <= int(L1_Percent):\n",
    "                        class_sml = [readclass,peakclass_peak,L1_class_peak,peakclass_sml,L1_class_sml,peakclass_lrg,L1_class_lrg,\"high\"]    \n",
    "                    dst = \"\".join(class_sml)\n",
    "                    if not os.path.exists(os.path.join(basepath, \"BLK\", dst)):\n",
    "                        os.makedirs(os.path.join(basepath, \"BLK\", dst))\n",
    "                    if not os.path.isfile(os.path.join(basepath, \"BLK\", dst,filename)):\n",
    "                        shutil.move(os.path.join(basepath, cell, filename), os.path.join(basepath, \"BLK\", dst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python2)",
   "language": "python",
   "name": "conda_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
