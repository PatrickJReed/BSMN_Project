{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, gc\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from time import time\n",
    "from subprocess import (call, Popen, PIPE)\n",
    "from itertools import product\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "from sklearn.decomposition import (PCA, RandomizedPCA)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import shutil\n",
    "import pybedtools\n",
    "import pysam\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "##Path to Data\n",
    "basepath = \"/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/\" \n",
    "diry = \"final\"\n",
    "narrowpeak = \"-ready_peaks.narrowPeak\"\n",
    "peaks_merged = \"_peaksMerged.txt\"\n",
    "peaks_merged_bed = \"_peaksMerged.bed\"\n",
    "peaks_correct_bed = \"_peaksCorrect.bed\"\n",
    "peakregions_sml = \".peakregions_sml\"\n",
    "peakregions_lrg = \".peakregions_lrg\"\n",
    "peaks_correct_data = \"_peaksCorrect.data\"\n",
    "loci_sml = \".loci_sml\"\n",
    "loci_lrg = \".loci_lrg\"\n",
    "overlap = \"_overlap_\"\n",
    "overlap_sml = \"_overlap_sml_\"\n",
    "overlap_lrg = \"_overlap_lrg_\"\n",
    "bam = \"-ready.bam\"\n",
    "igv = \"-igv.xml\"\n",
    "bed = \".bed\"\n",
    "\n",
    "Bulk_Brain = \"04132016_mw_Bulk_cor\"\n",
    "Bulk_Fibro = \"05252016_mw_Bulk_fib\"\n",
    "SC_MDA_1 = \"04132016_mw_1571_SC_B4_S48\"\n",
    "SC_MDA_2 = \"04132016_mw_1571_SC_D4_S49\"\n",
    "SC_MDA_3 = \"04132016_mw_L1B1_SC_A2_S43\"\n",
    "SC_MDA_4 = \"04132016_mw_L1B1_SC_C1_S45\"\n",
    "SC_MDA_5 = \"04132016_mw_L1B1_SC_C2_S46\"\n",
    "SC_MDA_6 = \"04132016_mw_L1B1_SC_D2_S50\"\n",
    "SC_MDA_7 = \"04132016_mw_L1B1_SC_E2_S51\"\n",
    "SC_MDA_8 = \"04132016_mw_L1B1_SC_E3_S52\"\n",
    "SC_MDA_9 = \"04132016_mw_L1B1_SC_F2_S53\"\n",
    "SC_MDA_10 = \"04132016_mw_L1B1_SC_G1_S54\"\n",
    "SC_MDA_11 = \"04132016_mw_L1B1_SC_H1_S55\"\n",
    "SC_MDA_12 = \"05252016_mw_L1B1_SC_B4_S47\"\n",
    "SC_MALBAC_1 = \"2122_S1\"\n",
    "SC_MALBAC_2 = \"2178_S2\"\n",
    "SC_MALBAC_3 = \"2179_S3\"\n",
    "SC_MALBAC_4 = \"2180_S4\"\n",
    "SC_MALBAC_5 = \"2184_S5\"\n",
    "SC_MALBAC_6 = \"2186_S6\"\n",
    "SC_MALBAC_7 = \"2187_S7\"\n",
    "SC_MALBAC_8 = \"2188_S8\"\n",
    "SC_MALBAC_9 = \"2193_S9\"\n",
    "SC_MALBAC_10 = \"2196_S10\"\n",
    "SC_MALBAC_11 = \"2197_S11\"\n",
    "SC_MALBAC_12 = \"2198_S12\"\n",
    "SC_MALBAC_13 = \"2261_S13\"\n",
    "SC_MALBAC_14 = \"2263_S14\"\n",
    "SC_MALBAC_15 = \"2264_S15\"\n",
    "SC_MALBAC_16 = \"2265_S16\"\n",
    "\n",
    "## rmask Paths \n",
    "L1HS = \"/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/rmask_L1HS_Final.bed\"\n",
    "L1PA2345 = \"/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/rmask_L1PA2345_Final.bed\"\n",
    "L1_Other = \"/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/rmask_L1_Other_Final.bed\"\n",
    "##IGV Template\n",
    "IGV = \"/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/igv-template3.xml\"\n",
    "\n",
    "def wait_timeout(proc, seconds):\n",
    "    \"\"\"Wait for a process to finish, or raise exception after timeout\"\"\"\n",
    "    start = time.time()\n",
    "    end = start + seconds\n",
    "    interval = min(seconds / 1000.0, .25)\n",
    "\n",
    "    while True:\n",
    "        result = proc.poll()\n",
    "        if result is not None:\n",
    "            return result\n",
    "        if time.time() >= end:\n",
    "            proc.kill()\n",
    "            #raise RuntimeError(\"Process timed out\")\n",
    "        time.sleep(interval)\n",
    "        \n",
    "##look at SC peaks differenatially present compared to bulk brain \n",
    "SingleCell_MALBAC = [SC_MALBAC_1,SC_MALBAC_2,SC_MALBAC_3,SC_MALBAC_4,SC_MALBAC_5,SC_MALBAC_6,SC_MALBAC_7,SC_MALBAC_8,SC_MALBAC_9,SC_MALBAC_10,SC_MALBAC_11,SC_MALBAC_12,SC_MALBAC_13,SC_MALBAC_14,SC_MALBAC_15,SC_MALBAC_16]\n",
    "#,SC_MALBAC_1,SC_MALBAC_2,SC_MALBAC_3,SC_MALBAC_4,SC_MALBAC_5,SC_MALBAC_6,SC_MALBAC_7,SC_MALBAC_8,SC_MALBAC_9,SC_MALBAC_10,SC_MALBAC_11,SC_MALBAC_12,SC_MALBAC_13,SC_MALBAC_14,SC_MALBAC_15,SC_MALBAC_16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BulkTissue = [Bulk_Brain, Bulk_Fibro]\n",
    "for tissue in BulkTissue:\n",
    "    os.chdir(os.path.join(basepath, diry, tissue))\n",
    "    name = os.path.join(basepath, diry, tissue, tissue)\n",
    "    bamfile = os.path.join(basepath, diry, tissue, tissue+bam)\n",
    "    #Call Peaks macs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in SingleCell_MALBAC:\n",
    "    print cell\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "    name = os.path.join(basepath, diry, cell, cell)\n",
    "    bamfile = os.path.join(basepath, diry, cell, cell+bam)\n",
    "    #call peaks macs2\n",
    "    p3 = Popen(['/home/preed/homer/bin/mergePeaks', '-code', '-prefix', cell, os.path.join(basepath, diry, cell, cell+narrowpeak), os.path.join(basepath, diry, Bulk_Brain, Bulk_Brain+narrowpeak), os.path.join(basepath, diry, Bulk_Fibro, Bulk_Fibro+narrowpeak)])\n",
    "    p3.wait()\n",
    "    myoutput = open(os.path.join(basepath, diry, cell, cell + peaks_merged), 'w')\n",
    "    p4 = Popen(['/home/preed/homer/bin/mergePeaks', os.path.join(basepath, diry, cell, cell+\"_100\"), os.path.join(basepath, diry, cell, cell+\"_110\"), os.path.join(basepath, diry, cell, cell+\"_101\"), os.path.join(basepath, diry, cell, cell+\"_010\")], stdout=myoutput)\n",
    "    p4.wait()\n",
    "    p5 = Popen(['/home/preed/homer/bin/pos2bed.pl', os.path.join(basepath, diry, cell, cell + peaks_merged), '-o', os.path.join(basepath, diry, cell, cell+peaks_merged_bed)])\n",
    "    p5.wait()\n",
    "    \n",
    "#Regional Encoding, nnn:nnn:n:nnn peaks found in SC,Brain,Fibro, reads found in SC, Brain, Fibro, Control or BG, overlap with L1HS,PA,or other\n",
    "#add number of peaks in sml reiogn and lrg region,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in SingleCell_MALBAC:\n",
    "    print cell\n",
    "    tree = ET.parse(IGV)\n",
    "    root = tree.getroot()\n",
    "    root[0][0].set('path', os.path.join(basepath, diry, cell, cell + bam)) #SC Path\n",
    "    root[0][2].set('path', os.path.join(basepath, diry, Bulk_Brain, Bulk_Brain + bam)) #Bulk Brain Path\n",
    "    root[0][5].set('path', os.path.join(basepath, diry, Bulk_Fibro, Bulk_Fibro + bam)) #Bulk Fib Path\n",
    "    root[1][0].set('id', os.path.join(basepath, diry, cell, cell + bam)) #SC Path\n",
    "    root[2][0].set('id', os.path.join(basepath, diry, Bulk_Brain, Bulk_Brain + bam)) #Bulk Brain path\n",
    "    root[3][0].set('id', os.path.join(basepath, diry, Bulk_Fibro, Bulk_Fibro + bam)) #Bulk Fib Path\n",
    "    tree.write(os.path.join(basepath, diry, cell, cell + igv))\n",
    "    \n",
    "    \n",
    "\n",
    "    myinput = open(os.path.join(basepath, diry, cell, cell + peaks_merged_bed))\n",
    "    myoutput = open(os.path.join(basepath, diry, cell, cell + peaks_correct_bed), 'w')\n",
    "    proc3 = Popen(['grep', '-E', '^(1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|X|Y)'], stdin=myinput, stdout=myoutput)\n",
    "    proc3.wait()    \n",
    "    \n",
    "    myinput = os.path.join(basepath, diry, cell, cell + peaks_correct_bed)\n",
    "    myoutput1 = os.path.join(basepath, diry, cell, cell + peakregions_sml)\n",
    "    myoutput2 = os.path.join(basepath, diry, cell, cell + peakregions_lrg)\n",
    "    with open(myoutput1, 'w') as outfile:\n",
    "        with open(myinput, 'r') as infile:\n",
    "            data = infile.readlines()\n",
    "            for string in data:\n",
    "                line = string.split('\\t')\n",
    "                pos1 = int(line[1])\n",
    "                pos2 = int(line[2])                  \n",
    "                center = int((pos1 + pos2)/2)\n",
    "                pad = 2500\n",
    "                start = center - pad\n",
    "                end = center + pad\n",
    "                row = [line[0], str(start), str(end)]\n",
    "                outfile.write('\\t'.join(row) + '\\n')\n",
    "    outfile.close()\n",
    "    infile.close()   \n",
    "    with open(myoutput2, 'w') as outfile:\n",
    "        with open(myinput, 'r') as infile:\n",
    "            data = infile.readlines()\n",
    "            for string in data:\n",
    "                line = string.split('\\t')\n",
    "                pos1 = int(line[1])\n",
    "                pos2 = int(line[2])                  \n",
    "                center = int((pos1 + pos2)/2)\n",
    "                pad = 10000\n",
    "                newstart = center - pad\n",
    "                newend = center + pad\n",
    "                row = [line[0], str(newstart), str(newend)]\n",
    "                outfile.write('\\t'.join(row) + '\\n')\n",
    "    outfile.close()\n",
    "    infile.close()\n",
    "    \n",
    "    \n",
    "    sc_file = pysam.AlignmentFile(os.path.join(basepath, diry, cell, cell + bam), \"rb\")\n",
    "    bb_file = pysam.AlignmentFile(os.path.join(basepath, diry, Bulk_Brain, Bulk_Brain + bam), \"rb\")\n",
    "    bf_file = pysam.AlignmentFile(os.path.join(basepath, diry, Bulk_Fibro, Bulk_Fibro + bam), \"rb\")\n",
    "\n",
    "    myinput = os.path.join(basepath, diry, cell, cell + peaks_correct_bed)\n",
    "    myoutput = os.path.join(basepath, diry, cell, cell + peaks_correct_data)\n",
    "    with open(myoutput, 'w') as outfile:\n",
    "        with open(myinput, 'r') as infile:\n",
    "            data = infile.readlines()\n",
    "            for region in data:\n",
    "                sc_iter = sc_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                bb_iter = bb_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                bf_iter = bf_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "                sc_i = 0\n",
    "                bb_i = 0\n",
    "                bf_i = 0\n",
    "                for x in sc_iter: sc_i+=1\n",
    "                for y in bb_iter: bb_i+=1\n",
    "                for z in bf_iter: bf_i+=1\n",
    "                sc_count = 1 if sc_i > 0 else 0\n",
    "                bb_count = 1 if bb_i > 0 else 0 \n",
    "                bf_count = 1 if bf_i > 0 else 0 \n",
    "                row = [str(region.strip().split('\\t')[0]), str(region.strip().split('\\t')[1]), str(region.strip().split('\\t')[2])]  \n",
    "                outfile.write('\\t'.join(row) +'\\t'+str(sc_count)+str(bb_count)+str(bf_count)+'\\n')\n",
    "   \n",
    "    filelist =[os.path.join(basepath, diry, cell, cell+narrowpeak),os.path.join(basepath, diry, Bulk_Brain, Bulk_Brain+narrowpeak),os.path.join(basepath, diry, Bulk_Fibro, Bulk_Fibro+narrowpeak),L1HS,L1PA2345,L1_Other]\n",
    "    a = pybedtools.BedTool(os.path.join(basepath, diry, cell, cell + peaks_correct_bed)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "    count = 0\n",
    "    for fname in filelist:\n",
    "        b = pybedtools.BedTool(fname)\n",
    "        a_and_b = a.intersect(b, c=True)\n",
    "        myoutput = os.path.join(basepath, diry, cell, cell + overlap + str(count))\n",
    "        count +=1\n",
    "        a_and_b.saveas(myoutput)\n",
    "        myinput = open(myoutput)\n",
    "        newoutput = open(myoutput+\"_binary\", 'w')\n",
    "        #overlap_append\n",
    "        awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($7 ~ \"^[1-9]*$\") $7 = \"1\"; else $7 = $7; }; 1\"\"\"\n",
    "        proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "        proc.wait()\n",
    "        newoutput.flush()\n",
    "    \n",
    "    a_sml = pybedtools.BedTool(os.path.join(basepath, diry, cell, cell + peakregions_sml)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "    count = 0\n",
    "    for fname in filelist:\n",
    "        b = pybedtools.BedTool(fname)\n",
    "        a_and_b = a_sml.intersect(b, c=True)\n",
    "        myoutput = os.path.join(basepath, diry, cell, cell + overlap_sml + str(count))\n",
    "        count +=1\n",
    "        a_and_b.saveas(myoutput)\n",
    "        myinput = open(myoutput)\n",
    "        newoutput = open(myoutput+\"_binary\", 'w')\n",
    "        #overlap_append\n",
    "        awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "        proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "        proc.wait()\n",
    "        newoutput.flush()\n",
    "    \n",
    "    a_lrg = pybedtools.BedTool(os.path.join(basepath, diry, cell, cell + peakregions_lrg)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "    count = 0\n",
    "    for fname in filelist:\n",
    "        b = pybedtools.BedTool(fname)\n",
    "        a_and_b = a_lrg.intersect(b, c=True)\n",
    "        myoutput = os.path.join(basepath, diry, cell, cell + overlap_lrg + str(count))\n",
    "        count +=1\n",
    "        a_and_b.saveas(myoutput)\n",
    "        myinput = open(myoutput)\n",
    "        newoutput = open(myoutput+\"_binary\", 'w')\n",
    "        #overlap_append\n",
    "        awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "        proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "        proc.wait()\n",
    "        newoutput.flush()        \n",
    "    \n",
    "    myinput_sml = os.path.join(basepath, diry, cell, cell + peakregions_sml)\n",
    "    myoutput_sml = os.path.join(basepath, diry, cell, cell + loci_sml)\n",
    "    with open(myoutput_sml, 'w') as outfile:\n",
    "        with open(myinput_sml, 'r') as infile:\n",
    "            data = infile.readlines()\n",
    "            for region in data:\n",
    "                row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "                outfile.write(\"\".join(row)+'\\n')\n",
    "    \n",
    "    myinput_lrg = os.path.join(basepath, diry, cell, cell + peakregions_lrg)\n",
    "    myoutput_lrg = os.path.join(basepath, diry, cell, cell + loci_lrg)\n",
    "    with open(myoutput_lrg, 'w') as outfile:\n",
    "        with open(myinput_lrg, 'r') as infile:\n",
    "            data = infile.readlines()\n",
    "            for region in data:\n",
    "                row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "                outfile.write(\"\".join(row)+'\\n')    \n",
    "\n",
    "    Popen(['split', '-l', '50', '-d', os.path.join(basepath, diry, cell, cell + loci_sml), os.path.join(basepath, diry, cell, cell + \".split_loci_sml_\")]).wait()\n",
    "    Popen(['split', '-l', '50', '-d', os.path.join(basepath, diry, cell, cell + loci_lrg), os.path.join(basepath, diry, cell, cell + \".split_loci_lrg_\")]).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in SingleCell_MALBAC:\n",
    "    print os.path.join(basepath, diry, cell)\n",
    "    os.chdir(os.path.join(basepath, diry, cell))    \n",
    "    locifile = os.path.join(basepath, diry, cell, cell + loci_sml)\n",
    "    worklist = glob.glob(\"*.split_loci_sml_*\")\n",
    "    batchsize = 10\n",
    "    print len(worklist)\n",
    "    for i in xrange(0, len(worklist), batchsize):\n",
    "        batch = worklist[i:i+batchsize]\n",
    "        print i\n",
    "        index = 1\n",
    "        procs = []\n",
    "        for file in batch:\n",
    "            print file\n",
    "            with open(os.path.join(basepath, diry, cell, file)) as f0:\n",
    "                first = f0.readline()# Read the first line.\n",
    "                for last in f0: pass\n",
    "                firstpic = cell+\"_sml\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                lastpic = cell+\"_sml\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                if not (glob.glob(os.path.join(basepath, diry, cell, firstpic)) or glob.glob(os.path.join(basepath, diry, cell, lastpic))): \n",
    "                    p = Popen(['igv_plotter', '-o', cell+\"_sml_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, diry, cell, cell + igv)])\n",
    "                    procs.append(p)\n",
    "        for pp in procs:\n",
    "            pp.wait()\n",
    "            #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in SingleCell_MALBAC:\n",
    "    print os.path.join(basepath, diry, cell)\n",
    "    os.chdir(os.path.join(basepath, diry, cell))    \n",
    "    locifile = os.path.join(basepath, diry, cell, cell + loci_lrg)\n",
    "    worklist = glob.glob(\"*.split_loci_lrg_*\")\n",
    "    batchsize = 10\n",
    "    print len(worklist)\n",
    "    for i in xrange(0, len(worklist), batchsize):\n",
    "        batch = worklist[i:i+batchsize]\n",
    "        print i\n",
    "        index = 1\n",
    "        procs = []\n",
    "        for file in batch:\n",
    "            print file\n",
    "            with open(os.path.join(basepath, diry, cell, file)) as f0:\n",
    "                first = f0.readline()# Read the first line.\n",
    "                for last in f0: pass\n",
    "                firstpic = cell+\"_lrg\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                lastpic = cell+\"_lrg\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                if not (glob.glob(os.path.join(basepath, diry, cell, firstpic)) or glob.glob(os.path.join(basepath, diry, cell, lastpic))): \n",
    "                    p = Popen(['igv_plotter', '-o', cell+\"_lrg_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/Common_Experiment/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, diry, cell, cell + igv)])\n",
    "                    procs.append(p)\n",
    "        for pp in procs:\n",
    "            pp.wait()\n",
    "            #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in SingleCell_MALBAC:\n",
    "    print cell\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "    for file in glob.glob(\"*s*__*.png\"):\n",
    "        newfile = re.sub(\"_s\\d+__\", \"-\", file)\n",
    "        shutil.move(file, newfile)     \n",
    "    for file in glob.glob(\"*.png\"):\n",
    "        img = Image.open(file)\n",
    "        width = img.size[0]\n",
    "        height = img.size[1]\n",
    "        img2 = img.crop((70,130,width,height)).resize((200,500))\n",
    "        path = os.path.splitext(file)[0]\n",
    "        basename = os.path.basename(path)\n",
    "        outfile1 = basename + \"_crp.png\"\n",
    "        img2.save(outfile1)\n",
    "        #os.remove(file)\n",
    "    \n",
    "    mergedpeak_data = os.path.join(basepath, diry, cell, cell + peaks_correct_data)\n",
    "    regions_sml = os.path.join(basepath, diry, cell, cell + peakregions_sml)\n",
    "    regions_lrg = os.path.join(basepath, diry, cell, cell + peakregions_lrg)\n",
    "      \n",
    "    count=1\n",
    "    with open(mergedpeak_data) as r0,open(regions_sml) as r_sml,open(regions_lrg) as r_lrg:\n",
    "        Files= {}\n",
    "        for rr0,rr_sml,rr_lrg in zip(r0,r_sml,r_lrg):\n",
    "            line = rr0.strip().split('\\t')[0]+\"\\t\"+rr0.strip().split('\\t')[1]+\"\\t\"+rr0.strip().split('\\t')[2]+\"\\t\"+cell+\"_sml-\"+rr_sml.strip().split('\\t')[0]+\"_\"+rr_sml.strip().split('\\t')[1]+\"_\"+rr_sml.strip().split('\\t')[2]+\"_crp.png\"+\"\\t\"+cell+\"_lrg-\"+rr_lrg.strip().split('\\t')[0]+\"_\"+rr_lrg.strip().split('\\t')[1]+\"_\"+rr_lrg.strip().split('\\t')[2]+\"_crp.png\"+\"\\t\"+rr0.strip().split('\\t')[3]\n",
    "            Files[str(count)] = line\n",
    "            count+=1\n",
    "    \n",
    "    a = os.path.join(basepath, diry, cell, cell+\"_overlap_0_binary\")\n",
    "    b = os.path.join(basepath, diry, cell, cell+\"_overlap_1_binary\")\n",
    "    c = os.path.join(basepath, diry, cell, cell+\"_overlap_2_binary\")\n",
    "    d = os.path.join(basepath, diry, cell, cell+\"_overlap_3_binary\")\n",
    "    e = os.path.join(basepath, diry, cell, cell+\"_overlap_4_binary\")\n",
    "    f = os.path.join(basepath, diry, cell, cell+\"_overlap_5_binary\")\n",
    "    count=1\n",
    "    with open(a) as f1,open(b) as f2,open(c) as f3,open(d) as f4,open(e) as f5,open(f) as f6:\n",
    "        Peaks = {}\n",
    "        for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "            line = aa.strip().split('\\t')[6]+bb.strip().split('\\t')[6]+cc.strip().split('\\t')[6]+\"\\t\"+dd.strip().split('\\t')[6]+ee.strip().split('\\t')[6]+ff.strip().split('\\t')[6]\n",
    "            Peaks[str(count)] = line\n",
    "            count+=1\n",
    "    \n",
    "    a_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_0_binary\")\n",
    "    b_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_1_binary\")\n",
    "    c_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_2_binary\")\n",
    "    d_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_3_binary\")\n",
    "    e_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_4_binary\")\n",
    "    f_sml = os.path.join(basepath, diry, cell, cell+\"_overlap_sml_5_binary\")\n",
    "    count=1\n",
    "    with open(a_sml) as f1,open(b_sml) as f2,open(c_sml) as f3,open(d_sml) as f4,open(e_sml) as f5,open(f_sml) as f6:\n",
    "        Small = {}\n",
    "        for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "            line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "            Small[str(count)] = line\n",
    "            count+=1\n",
    "    \n",
    "    a_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_0_binary\")\n",
    "    b_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_1_binary\")\n",
    "    c_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_2_binary\")\n",
    "    d_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_3_binary\")\n",
    "    e_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_4_binary\")\n",
    "    f_lrg = os.path.join(basepath, diry, cell, cell+\"_overlap_lrg_5_binary\")\n",
    "    count=1\n",
    "    with open(a_lrg) as f1,open(b_lrg) as f2,open(c_lrg) as f3,open(d_lrg) as f4,open(e_lrg) as f5,open(f_lrg) as f6:\n",
    "        Large = {}\n",
    "        for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "            line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "            Large[str(count)] = line\n",
    "            count+=1\n",
    "    \n",
    "    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\"),\"w\") as f8:\n",
    "        for key in Files:\n",
    "            encoding = Files[key]+\":\"+Peaks[key].strip().split('\\t')[0]+\":\"+Large[key].strip().split('\\t')[0]+\":\"+Peaks[key].strip().split('\\t')[1]+\"\\n\"\n",
    "        #for (Fi, Fj),(Pi,Pj),(Si,Sj),(Li,Lj) in zip(Files.items(),Peaks.items(),Small.items(),Large.items()):\n",
    "            #encoding = Fj+\":\"+Pj.strip().split('\\t')[0]+\":\"+Sj.strip().split('\\t')[0]+\":\"+Lj.strip().split('\\t')[0]+\":\"+Pj.strip().split('\\t')[1]+\":\"+Sj.strip().split('\\t')[1]+\":\"+Lj.strip().split('\\t')[1]+\"\\n\"\n",
    "            f8.write(encoding)\n",
    "            \n",
    "#    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "#        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "#            if os.path.isfile(line[3]):\n",
    "#                filename = line[3]\n",
    "#                dst = line[5]\n",
    "#                if not os.path.exists(dst):\n",
    "#                    os.makedirs(os.path.join(basepath, diry, cell, dst))\n",
    "#                shutil.move(filename, os.path.join(basepath, diry, cell, dst))\n",
    "                       \n",
    "#    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "#        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "#            if os.path.isfile(line[4]):\n",
    "#                filename = line[4]\n",
    "#                dst = line[5]\n",
    "#                if not os.path.exists(dst):\n",
    "#                    os.makedirs(os.path.join(basepath, diry, cell, dst))\n",
    "#                shutil.move(filename, os.path.join(basepath, diry, cell, dst))                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_sml = []\n",
    "metadata_sml = []\n",
    "#metadata2_sml = []\n",
    "\n",
    "for cell in SingleCell_MALBAC:\n",
    "    print cell\n",
    "    #shutil.copy(os.path.join(basepath, \"final\", cell, cell+\"_Input_metadata.txt\"),os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\"))\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "## Load images and meta data from meta files\n",
    "    \n",
    "    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            if os.path.isfile(line[3]):\n",
    "                readclass = line[5].split(\":\")[0]\n",
    "                peakclass_sml = line[5].split(\":\")[1]               \n",
    "                L1_class = line[5].split(\":\")[3]\n",
    "                #print os.path.join(basepath, diry, cell, line[3])\n",
    "                #if peakclass_sml == readclass:\n",
    "                class_sml = [readclass,L1_class]\n",
    "                filelist_sml.append(os.path.join(basepath, diry, cell, line[3])) \n",
    "                metadata_sml.append(\"\".join(class_sml))\n",
    "                #metadata2_sml.append(\"\".join(L1_class))\n",
    "                \n",
    "os.chdir(os.path.join(basepath))\n",
    "X_sml = np.array([np.array(Image.open(fname)) for fname in filelist_sml])\n",
    "Y_sml = np.array(metadata_sml).astype(str)\n",
    "print X_sml.shape\n",
    "print Y_sml.size\n",
    "print len(np.unique(Y_sml))\n",
    "n_classes_sml = Y_sml.size\n",
    "n_samples_sml = len(X_sml)\n",
    "np.savez(\"MALBAC_Cells_sml\", X_sml=X_sml, Y_sml=Y_sml)\n",
    "\n",
    "#X2 = X.reshape((n_sam,-1))\n",
    "#print X2.shape\n",
    "#X = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Y_sml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_lrg = []\n",
    "metadata_lrg = []\n",
    "\n",
    "for cell in SingleCell_MALBAC:\n",
    "    print cell\n",
    "    #shutil.copy(os.path.join(basepath, \"final\", cell, cell+\"_Input_metadata.txt\"),os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\"))\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "## Load images and meta data from meta files\n",
    "    \n",
    "    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            if os.path.isfile(line[4]):\n",
    "                peakclass_lrg = line[5].split(\":\")[2]               \n",
    "                L1_class = line[5].split(\":\")[3]\n",
    "                #print os.path.join(basepath, diry, cell, line[3])\n",
    "                filelist_lrg.append(os.path.join(basepath, diry, cell, line[4]))\n",
    "                class_lrg = [peakclass_lrg,L1_class]\n",
    "                metadata_lrg.append(\"\".join(Class_lrg))\n",
    "                \n",
    "os.chdir(os.path.join(basepath))\n",
    "X_lrg = np.array([np.array(Image.open(fname)) for fname in filelist_lrg])\n",
    "Y_lrg = np.array(metadata_lrg).astype(str)\n",
    "print X_lrg.shape\n",
    "print Y_lrg.size\n",
    "print len(np.unique(Y_lrg))\n",
    "n_classes_lrg = Y_lrg.size\n",
    "n_samples_lrg = len(X_lrg)\n",
    "np.savez(\"MALBAC_Cells_lrg\", X_lrg=X_lrg, Y_lrg=Y_lrg)\n",
    "\n",
    "#X2 = X.reshape((n_sam,-1))\n",
    "#print X2.shape\n",
    "#X = None\n",
    "#np.savez(\"All_Cells_sml\", X2=X2, Y=Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Test Basic Convolution Model\n",
    "\n",
    "import random\n",
    "np.random.seed(123)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32, allow_soft_placement=True, device_count = {'CPU': 32})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Ydict_1 = {}\n",
    "Ydict_2 = {}\n",
    "Y_unique = np.unique(Y_sml)\n",
    "print Y_unique\n",
    "i=0\n",
    "for y_u in Y_unique:\n",
    "    Ydict_1[i] = y_u\n",
    "    Ydict_2[y_u] = i\n",
    "    i+=1\n",
    "print Ydict_2\n",
    "trainsize = 20000\n",
    "testsize = 10000\n",
    "train_test = random.sample(range(0, n_samples_sml),trainsize+testsize)\n",
    "train = train_test[0:trainsize]\n",
    "test = train_test[trainsize:trainsize+testsize]\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_train = X_sml[train,:]\n",
    "X_test = X_sml[test,:]\n",
    "Ytr = [Ydict_2[y_train] for y_train in Y_sml[train]]\n",
    "Yte = [Ydict_2[y_test] for y_test in Y_sml[test]]\n",
    "Y_train = np_utils.to_categorical(Ytr)\n",
    "Y_test = np_utils.to_categorical(Yte)\n",
    "\n",
    "num_categories = len(np.unique(Y_sml))\n",
    "print num_categories\n",
    "# 7. Define model architecture\n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3,3), activation='relu', data_format=\"channels_first\", input_shape=(3,500,200)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 500, 200)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 500, 200)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "score\n",
    "\n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n",
    "classes\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "classes.shape\n",
    "\n",
    "classes[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Test Basic Convolution Model\n",
    "\n",
    "import random\n",
    "np.random.seed(123)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32, allow_soft_placement=True, device_count = {'CPU': 32})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Ydict_1 = {}\n",
    "Ydict_2 = {}\n",
    "Y_unique = np.unique(Y1_1)\n",
    "print Y_unique\n",
    "i=0\n",
    "for y_u in Y_unique:\n",
    "    Ydict_1[i] = y_u\n",
    "    Ydict_2[y_u] = i\n",
    "    i+=1\n",
    "print Ydict_2\n",
    "trainsize = 20000\n",
    "testsize = 10000\n",
    "train_test = random.sample(range(0, n_sam),trainsize+testsize)\n",
    "train = train_test[0:trainsize]\n",
    "test = train_test[trainsize:trainsize+testsize]\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_train = X1[train,:]\n",
    "X_test = X1[test,:]\n",
    "Ytr = [Ydict_2[y_train] for y_train in Y1_1[train]]\n",
    "Yte = [Ydict_2[y_test] for y_test in Y1_1[test]]\n",
    "Y_train = np_utils.to_categorical(Ytr)\n",
    "Y_test = np_utils.to_categorical(Yte)\n",
    "\n",
    "num_categories = len(np.unique(Y1_1))\n",
    "print num_categories\n",
    "# 7. Define model architecture\n",
    "model = InceptionV3(weights='None', include_top=False)\n",
    "\n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 500, 200)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 500, 200)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=50, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "score\n",
    "\n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n",
    "classes\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "classes.shape\n",
    "\n",
    "classes[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
