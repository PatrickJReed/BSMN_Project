{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, gc\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from time import time\n",
    "from subprocess import (call, Popen, PIPE)\n",
    "from itertools import product\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble, discriminant_analysis, random_projection)\n",
    "from sklearn.decomposition import (PCA, RandomizedPCA)\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import shutil\n",
    "import pybedtools\n",
    "import pysam\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "##Path to Data\n",
    "basepath = \"/home/ubuntu/efs/SLAV_Data/\" \n",
    "narrowpeak = \"-ready_peaks.narrowPeak\"\n",
    "peaks_merged = \"_peaksMerged.txt\"\n",
    "peaks_merged_bed = \"_peaksMerged.bed\"\n",
    "peaks_correct_bed = \"_peaksCorrect.bed\"\n",
    "peakregions_sml = \".peakregions_sml\"\n",
    "peakregions_lrg = \".peakregions_lrg\"\n",
    "peaks_correct_data = \"_peaksCorrect.data\"\n",
    "peaks_L1HS_bedgraph = \"_peaks_L1HS_mapped.bedgraph\"\n",
    "loci_sml = \".loci_sml\"\n",
    "loci_lrg = \".loci_lrg\"\n",
    "overlap = \"_overlap_\"\n",
    "overlap_sml = \"_overlap_sml_\"\n",
    "overlap_lrg = \"_overlap_lrg_\"\n",
    "L1HS_bam = \"-L1HS_mapped.bam\"\n",
    "bam = \"-ready.bam\"\n",
    "igv = \"-igv.xml\"\n",
    "bed = \".bed\"\n",
    "## rmask Paths \n",
    "L1HS = \"/home/ubuntu/efs/SLAV_Data/rmask_L1HS_Final.bed\"\n",
    "L1PA2345 = \"/home/ubuntu/efs/SLAV_Data/rmask_L1PA2345_Final.bed\"\n",
    "L1_Other = \"/home/ubuntu/efs/SLAV_Data/rmask_L1_Other_Final.bed\"\n",
    "##IGV Template\n",
    "IGV = \"/home/ubuntu/efs/SLAV_Data/igv-template4.xml\"\n",
    "\n",
    "Bulk_1571_Cerebellum = \"1571_cereb_BT_40_L3\"\n",
    "Bulk_1571_Hippocampus = \"1571_hippo_BT_41_L3\"\n",
    "Bulk_1846_Cerebellum = \"1846_cereb_BT_13_L3\"\n",
    "Bulk_1846_Cortex = \"1846_cortex_BT_71_L3\"\n",
    "Bulk_1846_Hippocampus = \"1846_hippo_BT_19_L3\"\n",
    "Bulk_1846_Liver = \"1846_liver_BT_22_L3\"\n",
    "Bulk_5125_Cortex = \"5125_cortex_BT_122_L3\"\n",
    "Bulk_5125_Hippocampus = \"5125_hippo_BT_139_L3\"\n",
    "Bulk_5125_Liver = \"5125_liver_BT_164_L3\"\n",
    "\n",
    "SC_1571_Hippo = [\"1571_hippo_SC_43_L3\",\"1571_hippo_SC_45_L3\",\"1571_hippo_SC_46_L3\",\"1571_hippo_SC_47_L3\",\"1571_hippo_SC_48_L3\",\"1571_hippo_SC_50_L3\",\"1571_hippo_SC_51_L3\",\"1571_hippo_SC_52_L3\",\"1571_hippo_SC_53_L3\",\"1571_hippo_SC_55_L3\",\"1571_hippo_SC_56_L3\",\"1571_hippo_SC_57_L3\",\"1571_hippo_SC_58_L3\",\"1571_hippo_SC_59_L3\",\"1571_hippo_SC_61_L3\",\"1571_hippo_SC_62_L3\",\"1571_hippo_SC_63_L3\",\"1571_hippo_SC_64_L3\"]\n",
    "SC_1846_Cortex = [\"1846_cortex_SC_72_L3\",\"1846_cortex_SC_73_L3\",\"1846_cortex_SC_74_L3\",\"1846_cortex_SC_75_L3\",\"1846_cortex_SC_78_L3\",\"1846_cortex_SC_79_L3\",\"1846_cortex_SC_80_L3\",\"1846_cortex_SC_81_L3\",\"1846_cortex_SC_82_L3\",\"1846_cortex_SC_83_L3\",\"1846_cortex_SC_84_L3\",\"1846_cortex_SC_85_L3\",\"1846_cortex_SC_86_L3\"]\n",
    "SC_1846_Hippo = [\"1846_hippo_SC_100_L3\",\"1846_hippo_SC_101_L3\",\"1846_hippo_SC_102_L3\",\"1846_hippo_SC_103_L3\",\"1846_hippo_SC_104_L3\",\"1846_hippo_SC_105_L3\",\"1846_hippo_SC_106_L3\",\"1846_hippo_SC_107_L3\",\"1846_hippo_SC_108_L3\",\"1846_hippo_SC_109_L3\",\"1846_hippo_SC_110_L3\",\"1846_hippo_SC_111_L3\",\"1846_hippo_SC_112_L3\",\"1846_hippo_SC_113_L3\",\"1846_hippo_SC_88_L3\",\"1846_hippo_SC_89_L3\",\"1846_hippo_SC_90_L3\",\"1846_hippo_SC_91_L3\",\"1846_hippo_SC_92_L3\",\"1846_hippo_SC_93_L3\",\"1846_hippo_SC_94_L3\",\"1846_hippo_SC_95_L3\",\"1846_hippo_SC_99_L3\"]\n",
    "SC_5125_Cortex = [\"5125_cortex_SC_125_L3\",\"5125_cortex_SC_126_L3\",\"5125_cortex_SC_127_L3\",\"5125_cortex_SC_128_L3\",\"5125_cortex_SC_129_L3\",\"5125_cortex_SC_130_L3\",\"5125_cortex_SC_131_L3\",\"5125_cortex_SC_132_L3\",\"5125_cortex_SC_133_L3\",\"5125_cortex_SC_134_L3\",\"5125_cortex_SC_135_L3\",\"5125_cortex_SC_136_L3\",\"5125_cortex_SC_138_L3\"]\n",
    "SC_5125_Hippo = [\"5125_hippo_SC_140_L3\",\"5125_hippo_SC_141_L3\",\"5125_hippo_SC_142_L3\",\"5125_hippo_SC_143_L3\",\"5125_hippo_SC_144_L3\",\"5125_hippo_SC_145_L3\",\"5125_hippo_SC_147_L3\",\"5125_hippo_SC_149_L3\",\"5125_hippo_SC_150_L3\",\"5125_hippo_SC_151_L3\",\"5125_hippo_SC_152_L3\",\"5125_hippo_SC_153_L3\",\"5125_hippo_SC_154_L3\",\"5125_hippo_SC_155_L3\",\"5125_hippo_SC_156_L3\",\"5125_hippo_SC_157_L3\",\"5125_hippo_SC_158_L3\",\"5125_hippo_SC_159_L3\",\"5125_hippo_SC_160_L3\",\"5125_hippo_SC_161_L3\",\"5125_hippo_SC_162_L3\",\"5125_hippo_SC_163_L3\"]\n",
    "\n",
    "\n",
    "Data_Sets = []\n",
    "Data_Sets.append([SC_1571_Hippo,Bulk_1571_Hippocampus,Bulk_1571_Cerebellum])\n",
    "Data_Sets.append([SC_1846_Cortex,Bulk_1846_Cortex,Bulk_1846_Liver])\n",
    "Data_Sets.append([SC_1846_Hippo,Bulk_1846_Hippocampus,Bulk_1846_Liver])\n",
    "Data_Sets.append([SC_5125_Cortex,Bulk_5125_Cortex,Bulk_5125_Liver])\n",
    "Data_Sets.append([SC_5125_Hippo,Bulk_5125_Hippocampus,Bulk_5125_Liver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:\n",
    "    tissue = dset[1]\n",
    "    os.chdir(os.path.join(basepath, tissue))\n",
    "    if not (glob.glob(os.path.join(basepath, tissue, \"peaks.txt\"))):\n",
    "        print tissue\n",
    "        name = os.path.join(basepath, tissue, tissue)\n",
    "        bamfile = os.path.join(basepath, tissue, tissue+bam)\n",
    "        #p1 = Popen(['/home/preed/homer/bin/makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['/home/preed/homer/bin/findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['/home/preed/homer/bin/pos2bed.pl', '-bed', os.path.join(basepath, tissue, \"peaks.txt\")])\n",
    "        p4.wait()\n",
    "    tissue = dset[2]\n",
    "    os.chdir(os.path.join(basepath, tissue))\n",
    "    if not (glob.glob(os.path.join(basepath, tissue, \"peaks.txt\"))):\n",
    "        print tissue\n",
    "        name = os.path.join(basepath, tissue, tissue)\n",
    "        bamfile = os.path.join(basepath, tissue, tissue+bam)\n",
    "        #p1 = Popen(['/home/preed/homer/bin/makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['/home/preed/homer/bin/findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['/home/preed/homer/bin/pos2bed.pl', '-bed', os.path.join(basepath, tissue, \"peaks.txt\")])\n",
    "        p4.wait()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, cell))\n",
    "        name = os.path.join(basepath, cell, cell)\n",
    "        bamfile = os.path.join(basepath, cell, cell+bam)\n",
    "        #p1 = Popen(['/home/preed/homer/bin/makeTagDirectory', '.', bamfile, '-format', 'sam', '-keepAll', '-single'])\n",
    "        #p1.wait()\n",
    "        p2a = Popen(['/home/preed/homer/bin/findPeaks', '.', '-o', 'peaks.txt', '-style', 'dnase', '-F', '0', '-L', '0', '-C', '0', '-tagThreshold', '5'])\n",
    "        p2a.wait()\n",
    "        p4 = Popen(['/home/preed/homer/bin/pos2bed.pl', '-bed', os.path.join(basepath, cell, \"peaks.txt\")])\n",
    "        p4.wait()\n",
    "        p5 = Popen(['/home/preed/homer/bin/mergePeaks', '-code', '-prefix', cell, os.path.join(basepath, cell, \"peaks.txt\"), os.path.join(basepath, dset[1], \"peaks.txt\"), os.path.join(basepath,  dset[2], \"peaks.txt\")])\n",
    "        p5.wait()\n",
    "        #myoutput = open(os.path.join(basepath, cell, cell + peaks_merged), 'w')\n",
    "        #p5 = Popen(['/home/preed/homer/bin/mergePeaks', os.path.join(basepath, cell, \"peaks.txt\"), os.path.join(basepath, dset[1], \"peaks.txt\"), os.path.join(basepath,  dset[2], \"peaks.txt\")], stdout=myoutput)\n",
    "        #p5.wait()\n",
    "        myoutput = open(os.path.join(basepath, cell, cell + peaks_merged), 'w')\n",
    "        p6 = Popen(['/home/preed/homer/bin/mergePeaks', os.path.join(basepath,  cell, cell+\"_100\"),os.path.join(basepath,  cell, cell+\"_110\"),os.path.join(basepath,  cell, cell+\"_111\")], stdout=myoutput)\n",
    "        p6.wait()\n",
    "        p7 = Popen(['/home/preed/homer/bin/pos2bed.pl', os.path.join(basepath,  cell, cell + peaks_merged), '-o', os.path.join(basepath,  cell, cell+peaks_merged_bed)])\n",
    "        p7.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571_hippo_SC_43_L3\n",
      "1571_hippo_SC_45_L3\n",
      "1571_hippo_SC_46_L3\n",
      "1571_hippo_SC_47_L3\n",
      "1571_hippo_SC_48_L3\n",
      "1571_hippo_SC_50_L3\n",
      "1571_hippo_SC_51_L3\n",
      "1571_hippo_SC_52_L3\n",
      "1571_hippo_SC_53_L3\n",
      "1571_hippo_SC_55_L3\n",
      "1571_hippo_SC_56_L3\n",
      "1571_hippo_SC_57_L3\n",
      "1571_hippo_SC_58_L3\n",
      "1571_hippo_SC_59_L3\n",
      "1571_hippo_SC_61_L3\n",
      "1571_hippo_SC_62_L3\n",
      "1571_hippo_SC_63_L3\n",
      "1571_hippo_SC_64_L3\n",
      "1846_cortex_SC_72_L3\n",
      "1846_cortex_SC_73_L3\n",
      "1846_cortex_SC_74_L3\n",
      "1846_cortex_SC_75_L3\n",
      "1846_cortex_SC_78_L3\n",
      "1846_cortex_SC_79_L3\n",
      "1846_cortex_SC_80_L3\n",
      "1846_cortex_SC_81_L3\n",
      "1846_cortex_SC_82_L3\n",
      "1846_cortex_SC_83_L3\n",
      "1846_cortex_SC_84_L3\n",
      "1846_cortex_SC_85_L3\n",
      "1846_cortex_SC_86_L3\n",
      "1846_hippo_SC_100_L3\n",
      "1846_hippo_SC_101_L3\n",
      "1846_hippo_SC_102_L3\n",
      "1846_hippo_SC_103_L3\n",
      "1846_hippo_SC_104_L3\n",
      "1846_hippo_SC_105_L3\n",
      "1846_hippo_SC_106_L3\n",
      "1846_hippo_SC_107_L3\n",
      "1846_hippo_SC_108_L3\n",
      "1846_hippo_SC_109_L3\n",
      "1846_hippo_SC_110_L3\n",
      "1846_hippo_SC_111_L3\n",
      "1846_hippo_SC_112_L3\n",
      "1846_hippo_SC_113_L3\n",
      "1846_hippo_SC_88_L3\n",
      "1846_hippo_SC_89_L3\n",
      "1846_hippo_SC_90_L3\n",
      "1846_hippo_SC_91_L3\n",
      "1846_hippo_SC_92_L3\n",
      "1846_hippo_SC_93_L3\n",
      "1846_hippo_SC_94_L3\n",
      "1846_hippo_SC_95_L3\n",
      "1846_hippo_SC_99_L3\n",
      "5125_cortex_SC_125_L3\n",
      "5125_cortex_SC_126_L3\n",
      "5125_cortex_SC_127_L3\n",
      "5125_cortex_SC_128_L3\n",
      "5125_cortex_SC_129_L3\n",
      "5125_cortex_SC_130_L3\n",
      "5125_cortex_SC_131_L3\n",
      "5125_cortex_SC_132_L3\n",
      "5125_cortex_SC_133_L3\n",
      "5125_cortex_SC_134_L3\n",
      "5125_cortex_SC_135_L3\n",
      "5125_cortex_SC_136_L3\n",
      "5125_cortex_SC_138_L3\n",
      "5125_hippo_SC_140_L3\n",
      "5125_hippo_SC_141_L3\n",
      "5125_hippo_SC_142_L3\n",
      "5125_hippo_SC_143_L3\n",
      "5125_hippo_SC_144_L3\n",
      "5125_hippo_SC_145_L3\n",
      "5125_hippo_SC_147_L3\n",
      "5125_hippo_SC_149_L3\n",
      "5125_hippo_SC_150_L3\n",
      "5125_hippo_SC_151_L3\n",
      "5125_hippo_SC_152_L3\n",
      "5125_hippo_SC_153_L3\n",
      "5125_hippo_SC_154_L3\n",
      "5125_hippo_SC_155_L3\n",
      "5125_hippo_SC_156_L3\n",
      "5125_hippo_SC_157_L3\n",
      "5125_hippo_SC_158_L3\n",
      "5125_hippo_SC_159_L3\n",
      "5125_hippo_SC_160_L3\n",
      "5125_hippo_SC_161_L3\n",
      "5125_hippo_SC_162_L3\n",
      "5125_hippo_SC_163_L3\n"
     ]
    }
   ],
   "source": [
    "for dset in Data_Sets:\n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "#         tree = ET.parse(IGV)\n",
    "#         root = tree.getroot()\n",
    "#         root[0][0].set('path', os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)) #L1HS bedgraph\n",
    "#         root[0][1].set('path', os.path.join(basepath,  cell, cell + bam)) #SC Path\n",
    "#         root[0][3].set('path', os.path.join(basepath,  dset[1], dset[1] + bam)) #Bulk Brain Path\n",
    "#         root[0][6].set('path', os.path.join(basepath,  dset[2], dset[2] + bam)) #Bulk Fib Path\n",
    "#         root[1][0].set('id', os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)) #L1HS bedgraph\n",
    "#         root[2][0].set('id', os.path.join(basepath,  cell, cell + bam)) #SC Path\n",
    "#         root[3][0].set('id', os.path.join(basepath,  dset[1], dset[1] + bam)) #Bulk Brain path\n",
    "#         root[4][0].set('id', os.path.join(basepath,  dset[2], dset[2] + bam)) #Bulk Fib Path\n",
    "#         tree.write(os.path.join(basepath,  cell, cell + igv))\n",
    "\n",
    "\n",
    "\n",
    "#         myinput = open(os.path.join(basepath,  cell, cell + peaks_merged_bed))\n",
    "#         myoutput = open(os.path.join(basepath,  cell, cell + peaks_correct_bed), 'w')\n",
    "#         proc3 = Popen(['grep', '-E', '^(1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|X|Y)'], stdin=myinput, stdout=myoutput)\n",
    "#         proc3.wait()    \n",
    "\n",
    "#         myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "#         myoutput1 = os.path.join(basepath,  cell, cell + peakregions_sml)\n",
    "#         myoutput2 = os.path.join(basepath,  cell, cell + peakregions_lrg)\n",
    "#         with open(myoutput1, 'w') as outfile:\n",
    "#             with open(myinput, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for string in data:\n",
    "#                     line = string.split('\\t')\n",
    "#                     pos1 = int(line[1])\n",
    "#                     pos2 = int(line[2])                  \n",
    "#                     center = int((pos1 + pos2)/2)\n",
    "#                     pad = 1000\n",
    "#                     start = center - pad\n",
    "#                     end = center + pad\n",
    "#                     row = [line[0], str(start), str(end)]\n",
    "#                     outfile.write('\\t'.join(row) + '\\n')\n",
    "#         outfile.close()\n",
    "#         infile.close()   \n",
    "#         with open(myoutput2, 'w') as outfile:\n",
    "#             with open(myinput, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for string in data:\n",
    "#                     line = string.split('\\t')\n",
    "#                     pos1 = int(line[1])\n",
    "#                     pos2 = int(line[2])                  \n",
    "#                     center = int((pos1 + pos2)/2)\n",
    "#                     pad = 10000\n",
    "#                     newstart = center - pad\n",
    "#                     newend = center + pad\n",
    "#                     row = [line[0], str(newstart), str(newend)]\n",
    "#                     outfile.write('\\t'.join(row) + '\\n')\n",
    "#         outfile.close()\n",
    "#         infile.close()\n",
    " \n",
    "#         #make and define L1HS sub sam file here and L1HS read names list\n",
    "        \n",
    "               \n",
    "#         sc_file = pysam.AlignmentFile(os.path.join(basepath,  cell, cell + bam), \"rb\")\n",
    "#         bb_file = pysam.AlignmentFile(os.path.join(basepath,  dset[1], dset[1] + bam), \"rb\")\n",
    "#         bf_file = pysam.AlignmentFile(os.path.join(basepath,  dset[2], dset[2] + bam), \"rb\")\n",
    "        \n",
    "\n",
    "#         myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "#         myoutput = os.path.join(basepath,  cell, cell + peaks_correct_data)\n",
    "        \n",
    "#         with open(myoutput, 'w') as outfile:\n",
    "#             with open(myinput, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for region in data:\n",
    "#                     sc_iter = sc_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "#                     bb_iter = bb_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "#                     bf_iter = bf_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2]))\n",
    "#                     sc_i = 0\n",
    "#                     bb_i = 0\n",
    "#                     bf_i = 0\n",
    "#                     for x in sc_iter: sc_i+=1\n",
    "#                     for y in bb_iter: bb_i+=1\n",
    "#                     for z in bf_iter: bf_i+=1\n",
    "#                     sc_count = 1 if sc_i > 0 else 0\n",
    "#                     bb_count = 1 if bb_i > 0 else 0 \n",
    "#                     bf_count = 1 if bf_i > 0 else 0 \n",
    "#                     row = [str(region.strip().split('\\t')[0]), str(region.strip().split('\\t')[1]), str(region.strip().split('\\t')[2])]  \n",
    "#                     outfile.write('\\t'.join(row) +'\\t'+str(sc_count)+str(bb_count)+str(bf_count)+'\\n')\n",
    "        \n",
    "#         myinput = os.path.join(basepath,  cell, cell + peaks_correct_bed)\n",
    "#         myoutput = open(os.path.join(basepath, cell, cell + L1HS_sam), 'w')\n",
    "#         myoutput2 = os.path.join(basepath,  cell, cell + peaks_L1HS_bedgraph)\n",
    "#         p1 = Popen(['samtools', 'view', '-h', '-L', L1HS, os.path.join(basepath,  cell, cell + bam)], stdout=myoutput)\n",
    "#         p1.wait()\n",
    "        \n",
    "        \n",
    "#         L1HS_file = pysam.AlignmentFile(os.path.join(basepath,  cell, cell + L1HS_sam), 'r')\n",
    "#         L1HS_read_names =[]\n",
    "#         for read in L1HS_file.fetch(): \n",
    "#             read_name = str(x).split('\\t')[0]\n",
    "#             L1HS_read_names.append(read_name)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         with open(myoutput2, 'w') as outfile:\n",
    "#             with open(myinput, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for region in data:\n",
    "#                     read_names = []\n",
    "#                     for read in sc_file.fetch(region.split('\\t')[0], int(region.split('\\t')[1]), int(region.split('\\t')[2])):\n",
    "#                         read_name = str(read).split('\\t')[0]\n",
    "#                         read_names.append(read_name)\n",
    "#                     L1HS_overlap = list(set(read_names) & set(L1HS_read_names))\n",
    "#                     print len(L1HS_overlap)\n",
    "#                     percent = (len(L1HS_overlap) / len(read_names))*100\n",
    "#                     row = [str(region.strip().split('\\t')[0]), str(region.strip().split('\\t')[1]), str(region.strip().split('\\t')[2]), str(percent)]  \n",
    "#                     outfile.write('\\t'.join(row)+'\\n')\n",
    "\n",
    "        filelist =[os.path.join(basepath,  cell, \"peaks.bed\"),os.path.join(basepath,  dset[1], \"peaks.bed\"),os.path.join(basepath,  dset[2], \"peaks.bed\"),L1HS,L1PA2345,L1_Other]\n",
    "        a = pybedtools.BedTool(os.path.join(basepath, cell, cell + peaks_correct_bed)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "        count = 0\n",
    "        for fname in filelist:\n",
    "            b = pybedtools.BedTool(fname)\n",
    "            a_and_b = a.window(b, w=250, c=True)\n",
    "            myoutput = os.path.join(basepath,  cell, cell + overlap + str(count))\n",
    "            count +=1\n",
    "            a_and_b.saveas(myoutput)\n",
    "            myinput = open(myoutput)\n",
    "            newoutput = open(myoutput+\"_binary\", 'w')\n",
    "            #overlap_append\n",
    "            awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($7 ~ \"^[1-9]*$\") $7 = \"1\"; else $7 = $7; }; 1\"\"\"\n",
    "            proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "            proc.wait()\n",
    "            newoutput.flush()\n",
    "\n",
    "#         a_sml = pybedtools.BedTool(os.path.join(basepath,  cell, cell + peakregions_sml)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "#         count = 0\n",
    "#         for fname in filelist:\n",
    "#             b = pybedtools.BedTool(fname)\n",
    "#             a_and_b = a_sml.intersect(b, c=True)\n",
    "#             myoutput = os.path.join(basepath,  cell, cell + overlap_sml + str(count))\n",
    "#             count +=1\n",
    "#             a_and_b.saveas(myoutput)\n",
    "#             myinput = open(myoutput)\n",
    "#             newoutput = open(myoutput+\"_binary\", 'w')\n",
    "#             #overlap_append\n",
    "#             awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "#             proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "#             proc.wait()\n",
    "#             newoutput.flush()\n",
    "\n",
    "#         a_lrg = pybedtools.BedTool(os.path.join(basepath,  cell, cell + peakregions_lrg)) ##mergedpeaks2 overlap with loci window or with peak location???\n",
    "#         count = 0\n",
    "#         for fname in filelist:\n",
    "#             b = pybedtools.BedTool(fname)\n",
    "#             a_and_b = a_lrg.intersect(b, c=True)\n",
    "#             myoutput = os.path.join(basepath,  cell, cell + overlap_lrg + str(count))\n",
    "#             count +=1\n",
    "#             a_and_b.saveas(myoutput)\n",
    "#             myinput = open(myoutput)\n",
    "#             newoutput = open(myoutput+\"_binary\", 'w')\n",
    "#             #overlap_append\n",
    "#             awk_cmd = r\"\"\"BEGIN { OFS = \"\\t\"; }; { if ($4 >= 2) $4 = \"2\"; else $4 = $4; }; 1\"\"\"\n",
    "#             proc = Popen(['awk', awk_cmd], stdin=myinput, stdout=newoutput)  \n",
    "#             proc.wait()\n",
    "#             newoutput.flush()        \n",
    "\n",
    "#         myinput_sml = os.path.join(basepath,  cell, cell + peakregions_sml)\n",
    "#         myoutput_sml = os.path.join(basepath,  cell, cell + loci_sml)\n",
    "#         with open(myoutput_sml, 'w') as outfile:\n",
    "#             with open(myinput_sml, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for region in data:\n",
    "#                     row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "#                     outfile.write(\"\".join(row)+'\\n')\n",
    "\n",
    "#         myinput_lrg = os.path.join(basepath,  cell, cell + peakregions_lrg)\n",
    "#         myoutput_lrg = os.path.join(basepath,  cell, cell + loci_lrg)\n",
    "#         with open(myoutput_lrg, 'w') as outfile:\n",
    "#             with open(myinput_lrg, 'r') as infile:\n",
    "#                 data = infile.readlines()\n",
    "#                 for region in data:\n",
    "#                     row = [str(region.strip().split('\\t')[0]),\":\",str(region.strip().split('\\t')[1]),\"-\",str(region.strip().split('\\t')[2])]  \n",
    "#                     outfile.write(\"\".join(row)+'\\n')    \n",
    "\n",
    "#         Popen(['split', '-l', '100', '-d', os.path.join(basepath,  cell, cell + loci_sml), os.path.join(basepath,  cell, cell + \".split_loci_sml_\")]).wait()\n",
    "#         Popen(['split', '-l', '100', '-d', os.path.join(basepath,  cell, cell + loci_lrg), os.path.join(basepath,  cell, cell + \".split_loci_lrg_\")]).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, cell))    \n",
    "        locifile = os.path.join(basepath, cell, cell + loci_sml)\n",
    "        worklist = glob.glob(\"*.split_loci_sml_*\")\n",
    "        batchsize = 10\n",
    "        print len(worklist)\n",
    "        for i in xrange(0, len(worklist), batchsize):\n",
    "            batch = worklist[i:i+batchsize]\n",
    "            print i\n",
    "            index = 1\n",
    "            procs = []\n",
    "            for file in batch:\n",
    "                print file\n",
    "                with open(os.path.join(basepath, cell, file)) as f0:\n",
    "                    first = f0.readline()# Read the first line.\n",
    "                    for last in f0: pass\n",
    "                    firstpic = cell+\"_sml\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    lastpic = cell+\"_sml\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    if not (glob.glob(os.path.join(basepath, cell, firstpic)) or glob.glob(os.path.join(basepath, cell, lastpic))): \n",
    "                        p = Popen(['igv_plotter', '-o', cell+\"_sml_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, cell, cell + igv)])\n",
    "                        procs.append(p)\n",
    "            for pp in procs:\n",
    "                pp.wait()\n",
    "                #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "        os.chdir(os.path.join(basepath, diry, cell))    \n",
    "        locifile = os.path.join(basepath, diry, cell, cell + loci_lrg)\n",
    "        worklist = glob.glob(\"*.split_loci_lrg_*\")\n",
    "        batchsize = 10\n",
    "        print len(worklist)\n",
    "        for i in xrange(0, len(worklist), batchsize):\n",
    "            batch = worklist[i:i+batchsize]\n",
    "            print i\n",
    "            index = 1\n",
    "            procs = []\n",
    "            for file in batch:\n",
    "                print file\n",
    "                with open(os.path.join(basepath, diry, cell, file)) as f0:\n",
    "                    first = f0.readline()# Read the first line.\n",
    "                    for last in f0: pass\n",
    "                    firstpic = cell+\"_lrg\"+\"*\"+first.strip().split(':')[0]+\"_\"+first.strip().split(':')[1].split('-')[0]+\"_\"+first.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    lastpic = cell+\"_lrg\"+\"*\"+last.strip().split(':')[0]+\"_\"+last.strip().split(':')[1].split('-')[0]+\"_\"+last.strip().split(':')[1].split('-')[1]+\".png\"\n",
    "                    if not (glob.glob(os.path.join(basepath, diry, cell, firstpic)) or glob.glob(os.path.join(basepath, diry, cell, lastpic))): \n",
    "                        p = Popen(['igv_plotter', '-o', cell+\"_lrg_\", '-L', file, '-v', '--max-panel-height', '1000', '--igv-jar-path', '/raid/LOG-G/SEQ_ARCHIVE/Patrick_Reed/BSMN/IGV_2.4-rc6/igv.jar', '-m', '6G', '-g', 'hg19', os.path.join(basepath, diry, cell, cell + igv)])\n",
    "                        procs.append(p)\n",
    "            for pp in procs:\n",
    "                pp.wait()\n",
    "                #wait_timeout(pp,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571_hippo_SC_43_L3\n",
      "1571_hippo_SC_45_L3\n",
      "1571_hippo_SC_46_L3\n",
      "1571_hippo_SC_47_L3\n",
      "1571_hippo_SC_48_L3\n",
      "1571_hippo_SC_50_L3\n",
      "1571_hippo_SC_51_L3\n",
      "1571_hippo_SC_52_L3\n",
      "1571_hippo_SC_53_L3\n",
      "1571_hippo_SC_55_L3\n",
      "1571_hippo_SC_56_L3\n",
      "1571_hippo_SC_57_L3\n",
      "1571_hippo_SC_58_L3\n",
      "1571_hippo_SC_59_L3\n",
      "1571_hippo_SC_61_L3\n",
      "1571_hippo_SC_62_L3\n",
      "1571_hippo_SC_63_L3\n",
      "1571_hippo_SC_64_L3\n",
      "1846_cortex_SC_72_L3\n",
      "1846_cortex_SC_73_L3\n",
      "1846_cortex_SC_74_L3\n",
      "1846_cortex_SC_75_L3\n",
      "1846_cortex_SC_78_L3\n",
      "1846_cortex_SC_79_L3\n",
      "1846_cortex_SC_80_L3\n",
      "1846_cortex_SC_81_L3\n",
      "1846_cortex_SC_82_L3\n",
      "1846_cortex_SC_83_L3\n",
      "1846_cortex_SC_84_L3\n",
      "1846_cortex_SC_85_L3\n",
      "1846_cortex_SC_86_L3\n",
      "1846_hippo_SC_100_L3\n",
      "1846_hippo_SC_101_L3\n",
      "1846_hippo_SC_102_L3\n",
      "1846_hippo_SC_103_L3\n",
      "1846_hippo_SC_104_L3\n",
      "1846_hippo_SC_105_L3\n",
      "1846_hippo_SC_106_L3\n",
      "1846_hippo_SC_107_L3\n",
      "1846_hippo_SC_108_L3\n",
      "1846_hippo_SC_109_L3\n",
      "1846_hippo_SC_110_L3\n",
      "1846_hippo_SC_111_L3\n",
      "1846_hippo_SC_112_L3\n",
      "1846_hippo_SC_113_L3\n",
      "1846_hippo_SC_88_L3\n",
      "1846_hippo_SC_89_L3\n",
      "1846_hippo_SC_90_L3\n",
      "1846_hippo_SC_91_L3\n",
      "1846_hippo_SC_92_L3\n",
      "1846_hippo_SC_93_L3\n",
      "1846_hippo_SC_94_L3\n",
      "1846_hippo_SC_95_L3\n",
      "1846_hippo_SC_99_L3\n",
      "5125_cortex_SC_125_L3\n",
      "5125_cortex_SC_126_L3\n",
      "5125_cortex_SC_127_L3\n",
      "5125_cortex_SC_128_L3\n",
      "5125_cortex_SC_129_L3\n",
      "5125_cortex_SC_130_L3\n",
      "5125_cortex_SC_131_L3\n",
      "5125_cortex_SC_132_L3\n",
      "5125_cortex_SC_133_L3\n",
      "5125_cortex_SC_134_L3\n",
      "5125_cortex_SC_135_L3\n",
      "5125_cortex_SC_136_L3\n",
      "5125_cortex_SC_138_L3\n",
      "5125_hippo_SC_140_L3\n",
      "5125_hippo_SC_141_L3\n",
      "5125_hippo_SC_142_L3\n",
      "5125_hippo_SC_143_L3\n",
      "5125_hippo_SC_144_L3\n",
      "5125_hippo_SC_145_L3\n",
      "5125_hippo_SC_147_L3\n",
      "5125_hippo_SC_149_L3\n",
      "5125_hippo_SC_150_L3\n",
      "5125_hippo_SC_151_L3\n",
      "5125_hippo_SC_152_L3\n",
      "5125_hippo_SC_153_L3\n",
      "5125_hippo_SC_154_L3\n",
      "5125_hippo_SC_155_L3\n",
      "5125_hippo_SC_156_L3\n",
      "5125_hippo_SC_157_L3\n",
      "5125_hippo_SC_158_L3\n",
      "5125_hippo_SC_159_L3\n",
      "5125_hippo_SC_160_L3\n",
      "5125_hippo_SC_161_L3\n",
      "5125_hippo_SC_162_L3\n",
      "5125_hippo_SC_163_L3\n"
     ]
    }
   ],
   "source": [
    "for dset in Data_Sets:    \n",
    "    for cell in dset[0]:\n",
    "        print cell\n",
    "#         os.chdir(os.path.join(basepath, diry, cell))\n",
    "#         for file in glob.glob(\"*s*__*.png\"):\n",
    "#             newfile = re.sub(\"_s\\d+__\", \"-\", file)\n",
    "#             shutil.move(file, newfile)     \n",
    "#         for file in glob.glob(\"*.png\"):\n",
    "#             img = Image.open(file)\n",
    "#             width = img.size[0]\n",
    "#             height = img.size[1]\n",
    "#             img2 = img.crop((70,130,width,height)).resize((200,500))\n",
    "#             path = os.path.splitext(file)[0]\n",
    "#             basename = os.path.basename(path)\n",
    "#             outfile1 = basename + \"_crp.png\"\n",
    "#             img2.save(outfile1)\n",
    "#             #os.remove(file)\n",
    "\n",
    "        mergedpeak_data = os.path.join(basepath, cell, cell + peaks_correct_data)\n",
    "        regions_sml = os.path.join(basepath, cell, cell + peakregions_sml)\n",
    "        regions_lrg = os.path.join(basepath, cell, cell + peakregions_lrg)\n",
    "\n",
    "        count=1\n",
    "        with open(mergedpeak_data) as r0,open(regions_sml) as r_sml,open(regions_lrg) as r_lrg:\n",
    "            Files= {}\n",
    "            for rr0,rr_sml,rr_lrg in zip(r0,r_sml,r_lrg):\n",
    "                line = rr0.strip().split('\\t')[0]+\"\\t\"+rr0.strip().split('\\t')[1]+\"\\t\"+rr0.strip().split('\\t')[2]+\"\\t\"+cell+\"_sml-\"+rr_sml.strip().split('\\t')[0]+\"_\"+rr_sml.strip().split('\\t')[1]+\"_\"+rr_sml.strip().split('\\t')[2]+\"_cropped.png\"+\"\\t\"+cell+\"_lrg-\"+rr_lrg.strip().split('\\t')[0]+\"_\"+rr_lrg.strip().split('\\t')[1]+\"_\"+rr_lrg.strip().split('\\t')[2]+\"_cropped.png\"+\"\\t\"+rr0.strip().split('\\t')[3]\n",
    "                Files[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a = os.path.join(basepath, cell, cell+\"_overlap_0_binary\")\n",
    "        b = os.path.join(basepath, cell, cell+\"_overlap_1_binary\")\n",
    "        c = os.path.join(basepath, cell, cell+\"_overlap_2_binary\")\n",
    "        d = os.path.join(basepath, cell, cell+\"_overlap_3_binary\")\n",
    "        e = os.path.join(basepath, cell, cell+\"_overlap_4_binary\")\n",
    "        f = os.path.join(basepath, cell, cell+\"_overlap_5_binary\")\n",
    "        count=1\n",
    "        with open(a) as f1,open(b) as f2,open(c) as f3,open(d) as f4,open(e) as f5,open(f) as f6:\n",
    "            Peaks = {}\n",
    "            for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "                line = aa.strip().split('\\t')[6]+bb.strip().split('\\t')[6]+cc.strip().split('\\t')[6]+\"\\t\"+dd.strip().split('\\t')[6]+ee.strip().split('\\t')[6]+ff.strip().split('\\t')[6]\n",
    "                Peaks[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_0_binary\")\n",
    "        b_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_1_binary\")\n",
    "        c_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_2_binary\")\n",
    "        d_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_3_binary\")\n",
    "        e_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_4_binary\")\n",
    "        f_sml = os.path.join(basepath, cell, cell+\"_overlap_sml_5_binary\")\n",
    "        count=1\n",
    "        with open(a_sml) as f1,open(b_sml) as f2,open(c_sml) as f3,open(d_sml) as f4,open(e_sml) as f5,open(f_sml) as f6:\n",
    "            Small = {}\n",
    "            for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "                line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "                Small[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        a_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_0_binary\")\n",
    "        b_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_1_binary\")\n",
    "        c_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_2_binary\")\n",
    "        d_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_3_binary\")\n",
    "        e_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_4_binary\")\n",
    "        f_lrg = os.path.join(basepath, cell, cell+\"_overlap_lrg_5_binary\")\n",
    "        count=1\n",
    "        with open(a_lrg) as f1,open(b_lrg) as f2,open(c_lrg) as f3,open(d_lrg) as f4,open(e_lrg) as f5,open(f_lrg) as f6:\n",
    "            Large = {}\n",
    "            for aa,bb,cc,dd,ee,ff in zip(f1,f2,f3,f4,f5,f6):\n",
    "                line = aa.strip().split('\\t')[3]+bb.strip().split('\\t')[3]+cc.strip().split('\\t')[3]+\"\\t\"+dd.strip().split('\\t')[3]+ee.strip().split('\\t')[3]+ff.strip().split('\\t')[3]\n",
    "                Large[str(count)] = line\n",
    "                count+=1\n",
    "\n",
    "        with open(os.path.join(basepath, cell, cell+\"_Input_metadata.txt\"),\"w\") as f8:\n",
    "            for key in Files:\n",
    "                encoding = Files[key]+\":\"+Peaks[key].strip().split('\\t')[0]+\":\"+Peaks[key].strip().split('\\t')[1]+\":\"+Small[key].strip().split('\\t')[0]+\":\"+Small[key].strip().split('\\t')[1]+\":\"+Large[key].strip().split('\\t')[0]+\":\"+Large[key].strip().split('\\t')[1]+\"\\n\"\n",
    "            #for (Fi, Fj),(Pi,Pj),(Si,Sj),(Li,Lj) in zip(Files.items(),Peaks.items(),Small.items(),Large.items()):\n",
    "                #encoding = Fj+\":\"+Pj.strip().split('\\t')[0]+\":\"+Sj.strip().split('\\t')[0]+\":\"+Lj.strip().split('\\t')[0]+\":\"+Pj.strip().split('\\t')[1]+\":\"+Sj.strip().split('\\t')[1]+\":\"+Lj.strip().split('\\t')[1]+\"\\n\"\n",
    "                f8.write(encoding)\n",
    "\n",
    "    #    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "    #        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "    #            if os.path.isfile(line[3]):\n",
    "    #                filename = line[3]\n",
    "    #                dst = line[5]\n",
    "    #                if not os.path.exists(dst):\n",
    "    #                    os.makedirs(os.path.join(basepath, diry, cell, dst))\n",
    "    #                shutil.move(filename, os.path.join(basepath, diry, cell, dst))\n",
    "\n",
    "    #    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "    #        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "    #            if os.path.isfile(line[4]):\n",
    "    #                filename = line[4]\n",
    "    #                dst = line[5]\n",
    "    #                if not os.path.exists(dst):\n",
    "    #                    os.makedirs(os.path.join(basepath, diry, cell, dst))\n",
    "    #                shutil.move(filename, os.path.join(basepath, diry, cell, dst))                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_sml = []\n",
    "metadata_sml = []\n",
    "#metadata2_sml = []\n",
    "\n",
    "for cell in SingleCell_MDA:\n",
    "    print cell\n",
    "    #shutil.copy(os.path.join(basepath, \"final\", cell, cell+\"_Input_metadata.txt\"),os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\"))\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "## Load images and meta data from meta files\n",
    "    \n",
    "    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            if os.path.isfile(line[3]):\n",
    "                readclass = line[5].split(\":\")[0]\n",
    "                peakclass_sml = line[5].split(\":\")[1]               \n",
    "                L1_class = line[5].split(\":\")[3]\n",
    "                #print os.path.join(basepath, diry, cell, line[3])\n",
    "                #if peakclass_sml == readclass:\n",
    "                class_sml = [readclass,L1_class]\n",
    "                filelist_sml.append(os.path.join(basepath, diry, cell, line[3])) \n",
    "                metadata_sml.append(\"\".join(class_sml))\n",
    "                #metadata2_sml.append(\"\".join(L1_class))\n",
    "                \n",
    "os.chdir(os.path.join(basepath))\n",
    "X_sml = np.array([np.array(Image.open(fname)) for fname in filelist_sml])\n",
    "Y_sml = np.array(metadata_sml).astype(str)\n",
    "print X_sml.shape\n",
    "print Y_sml.size\n",
    "print len(np.unique(Y_sml))\n",
    "n_classes_sml = Y_sml.size\n",
    "n_samples_sml = len(X_sml)\n",
    "np.savez(\"MDA_Cells_sml\", X_sml=X_sml, Y_sml=Y_sml)\n",
    "\n",
    "#X2 = X.reshape((n_sam,-1))\n",
    "#print X2.shape\n",
    "#X = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Y_sml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_lrg = []\n",
    "metadata_lrg = []\n",
    "\n",
    "for cell in SingleCell_MDA:\n",
    "    print cell\n",
    "    #shutil.copy(os.path.join(basepath, \"final\", cell, cell+\"_Input_metadata.txt\"),os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\"))\n",
    "    os.chdir(os.path.join(basepath, diry, cell))\n",
    "## Load images and meta data from meta files\n",
    "    \n",
    "    with open(os.path.join(basepath, diry, cell, cell+\"_Input_metadata.txt\")) as f:\n",
    "        for line in csv.reader(f, delimiter=\"\\t\"):\n",
    "            if os.path.isfile(line[4]):\n",
    "                peakclass_lrg = line[5].split(\":\")[2]               \n",
    "                L1_class = line[5].split(\":\")[3]\n",
    "                #print os.path.join(basepath, diry, cell, line[3])\n",
    "                filelist_lrg.append(os.path.join(basepath, diry, cell, line[4]))\n",
    "                class_lrg = [peakclass_lrg,L1_class]\n",
    "                metadata_lrg.append(\"\".join(Class_lrg))\n",
    "                \n",
    "os.chdir(os.path.join(basepath))\n",
    "X_lrg = np.array([np.array(Image.open(fname)) for fname in filelist_lrg])\n",
    "Y_lrg = np.array(metadata_lrg).astype(str)\n",
    "print X_lrg.shape\n",
    "print Y_lrg.size\n",
    "print len(np.unique(Y_lrg))\n",
    "n_classes_lrg = Y_lrg.size\n",
    "n_samples_lrg = len(X_lrg)\n",
    "np.savez(\"MDA_Cells_lrg\", X_lrg=X_lrg, Y_lrg=Y_lrg)\n",
    "\n",
    "#X2 = X.reshape((n_sam,-1))\n",
    "#print X2.shape\n",
    "#X = None\n",
    "#np.savez(\"All_Cells_sml\", X2=X2, Y=Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Test Basic Convolution Model\n",
    "\n",
    "import random\n",
    "np.random.seed(123)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32, allow_soft_placement=True, device_count = {'CPU': 32})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Ydict_1 = {}\n",
    "Ydict_2 = {}\n",
    "Y_unique = np.unique(Y_sml)\n",
    "print Y_unique\n",
    "i=0\n",
    "for y_u in Y_unique:\n",
    "    Ydict_1[i] = y_u\n",
    "    Ydict_2[y_u] = i\n",
    "    i+=1\n",
    "print Ydict_2\n",
    "trainsize = 20000\n",
    "testsize = 10000\n",
    "train_test = random.sample(range(0, n_samples_sml),trainsize+testsize)\n",
    "train = train_test[0:trainsize]\n",
    "test = train_test[trainsize:trainsize+testsize]\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_train = X_sml[train,:]\n",
    "X_test = X_sml[test,:]\n",
    "Ytr = [Ydict_2[y_train] for y_train in Y_sml[train]]\n",
    "Yte = [Ydict_2[y_test] for y_test in Y_sml[test]]\n",
    "Y_train = np_utils.to_categorical(Ytr)\n",
    "Y_test = np_utils.to_categorical(Yte)\n",
    "\n",
    "num_categories = len(np.unique(Y_sml))\n",
    "print num_categories\n",
    "# 7. Define model architecture\n",
    "model = Sequential() \n",
    "model.add(Conv2D(32, (3,3), activation='relu', data_format=\"channels_first\", input_shape=(3,500,200)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 500, 200)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 500, 200)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "score\n",
    "\n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n",
    "classes\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "classes.shape\n",
    "\n",
    "classes[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Test Basic Convolution Model\n",
    "\n",
    "import random\n",
    "np.random.seed(123)  # for reproducibility\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32, allow_soft_placement=True, device_count = {'CPU': 32})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "# 6. Preprocess class labels\n",
    "Ydict_1 = {}\n",
    "Ydict_2 = {}\n",
    "Y_unique = np.unique(Y1_1)\n",
    "print Y_unique\n",
    "i=0\n",
    "for y_u in Y_unique:\n",
    "    Ydict_1[i] = y_u\n",
    "    Ydict_2[y_u] = i\n",
    "    i+=1\n",
    "print Ydict_2\n",
    "trainsize = 20000\n",
    "testsize = 10000\n",
    "train_test = random.sample(range(0, n_sam),trainsize+testsize)\n",
    "train = train_test[0:trainsize]\n",
    "test = train_test[trainsize:trainsize+testsize]\n",
    "X_train = []\n",
    "X_test = []\n",
    "X_train = X1[train,:]\n",
    "X_test = X1[test,:]\n",
    "Ytr = [Ydict_2[y_train] for y_train in Y1_1[train]]\n",
    "Yte = [Ydict_2[y_test] for y_test in Y1_1[test]]\n",
    "Y_train = np_utils.to_categorical(Ytr)\n",
    "Y_test = np_utils.to_categorical(Yte)\n",
    "\n",
    "num_categories = len(np.unique(Y1_1))\n",
    "print num_categories\n",
    "# 7. Define model architecture\n",
    "model = InceptionV3(weights='None', include_top=False)\n",
    "\n",
    "# 8. Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 500, 200)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 500, 200)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 9. Fit model on training data\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=50, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "score\n",
    "\n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n",
    "classes\n",
    "\n",
    "X_test.shape\n",
    "\n",
    "classes.shape\n",
    "\n",
    "classes[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
